{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 2.29, acc: 0.138, val_loss: 2.28, val_acc: 0.213\n",
      "epoch: 2, loss: 2.25, acc: 0.265, val_loss: 2.16, val_acc: 0.482\n",
      "epoch: 3, loss: 1.94, acc: 0.410, val_loss: 1.38, val_acc: 0.633\n",
      "epoch: 4, loss: 1.3, acc: 0.553, val_loss: 0.871, val_acc: 0.727\n",
      "epoch: 5, loss: 1.0, acc: 0.661, val_loss: 0.669, val_acc: 0.805\n",
      "epoch: 6, loss: 0.826, acc: 0.731, val_loss: 0.539, val_acc: 0.845\n",
      "epoch: 7, loss: 0.708, acc: 0.777, val_loss: 0.461, val_acc: 0.868\n",
      "epoch: 8, loss: 0.626, acc: 0.807, val_loss: 0.407, val_acc: 0.885\n",
      "epoch: 9, loss: 0.567, acc: 0.830, val_loss: 0.372, val_acc: 0.894\n",
      "epoch: 10, loss: 0.523, acc: 0.845, val_loss: 0.343, val_acc: 0.901\n",
      "epoch: 11, loss: 0.486, acc: 0.856, val_loss: 0.318, val_acc: 0.908\n",
      "epoch: 12, loss: 0.456, acc: 0.866, val_loss: 0.302, val_acc: 0.914\n",
      "epoch: 13, loss: 0.429, acc: 0.876, val_loss: 0.283, val_acc: 0.919\n",
      "epoch: 14, loss: 0.408, acc: 0.883, val_loss: 0.27, val_acc: 0.921\n",
      "epoch: 15, loss: 0.389, acc: 0.890, val_loss: 0.254, val_acc: 0.926\n",
      "epoch: 16, loss: 0.37, acc: 0.894, val_loss: 0.242, val_acc: 0.929\n",
      "epoch: 17, loss: 0.358, acc: 0.898, val_loss: 0.232, val_acc: 0.932\n",
      "epoch: 18, loss: 0.344, acc: 0.902, val_loss: 0.222, val_acc: 0.934\n",
      "epoch: 19, loss: 0.328, acc: 0.908, val_loss: 0.215, val_acc: 0.936\n",
      "epoch: 20, loss: 0.317, acc: 0.910, val_loss: 0.205, val_acc: 0.939\n",
      "epoch: 21, loss: 0.307, acc: 0.913, val_loss: 0.202, val_acc: 0.941\n",
      "epoch: 22, loss: 0.297, acc: 0.918, val_loss: 0.193, val_acc: 0.943\n",
      "epoch: 23, loss: 0.285, acc: 0.920, val_loss: 0.187, val_acc: 0.945\n",
      "epoch: 24, loss: 0.276, acc: 0.922, val_loss: 0.181, val_acc: 0.946\n",
      "epoch: 25, loss: 0.274, acc: 0.923, val_loss: 0.175, val_acc: 0.947\n",
      "epoch: 26, loss: 0.263, acc: 0.925, val_loss: 0.171, val_acc: 0.949\n",
      "epoch: 27, loss: 0.259, acc: 0.927, val_loss: 0.168, val_acc: 0.949\n",
      "epoch: 28, loss: 0.25, acc: 0.931, val_loss: 0.164, val_acc: 0.951\n",
      "epoch: 29, loss: 0.247, acc: 0.931, val_loss: 0.159, val_acc: 0.952\n",
      "epoch: 30, loss: 0.24, acc: 0.933, val_loss: 0.156, val_acc: 0.952\n",
      "epoch: 31, loss: 0.236, acc: 0.935, val_loss: 0.154, val_acc: 0.954\n",
      "epoch: 32, loss: 0.23, acc: 0.936, val_loss: 0.15, val_acc: 0.955\n",
      "epoch: 33, loss: 0.225, acc: 0.936, val_loss: 0.147, val_acc: 0.956\n",
      "epoch: 34, loss: 0.221, acc: 0.937, val_loss: 0.146, val_acc: 0.957\n",
      "epoch: 35, loss: 0.213, acc: 0.940, val_loss: 0.143, val_acc: 0.957\n",
      "epoch: 36, loss: 0.21, acc: 0.940, val_loss: 0.14, val_acc: 0.958\n",
      "epoch: 37, loss: 0.211, acc: 0.941, val_loss: 0.136, val_acc: 0.959\n",
      "epoch: 38, loss: 0.204, acc: 0.943, val_loss: 0.134, val_acc: 0.959\n",
      "epoch: 39, loss: 0.201, acc: 0.943, val_loss: 0.133, val_acc: 0.960\n",
      "epoch: 40, loss: 0.2, acc: 0.944, val_loss: 0.131, val_acc: 0.960\n",
      "epoch: 41, loss: 0.192, acc: 0.947, val_loss: 0.13, val_acc: 0.960\n",
      "epoch: 42, loss: 0.194, acc: 0.945, val_loss: 0.127, val_acc: 0.961\n",
      "epoch: 43, loss: 0.187, acc: 0.947, val_loss: 0.126, val_acc: 0.961\n",
      "epoch: 44, loss: 0.186, acc: 0.948, val_loss: 0.124, val_acc: 0.962\n",
      "epoch: 45, loss: 0.183, acc: 0.948, val_loss: 0.123, val_acc: 0.962\n",
      "epoch: 46, loss: 0.18, acc: 0.949, val_loss: 0.121, val_acc: 0.963\n",
      "epoch: 47, loss: 0.178, acc: 0.950, val_loss: 0.12, val_acc: 0.963\n",
      "epoch: 48, loss: 0.175, acc: 0.951, val_loss: 0.119, val_acc: 0.964\n",
      "epoch: 49, loss: 0.169, acc: 0.953, val_loss: 0.117, val_acc: 0.964\n",
      "epoch: 50, loss: 0.171, acc: 0.952, val_loss: 0.117, val_acc: 0.965\n",
      "epoch: 51, loss: 0.166, acc: 0.954, val_loss: 0.117, val_acc: 0.965\n",
      "epoch: 52, loss: 0.163, acc: 0.955, val_loss: 0.116, val_acc: 0.965\n",
      "epoch: 53, loss: 0.163, acc: 0.955, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 54, loss: 0.162, acc: 0.954, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 55, loss: 0.16, acc: 0.954, val_loss: 0.113, val_acc: 0.966\n",
      "epoch: 56, loss: 0.157, acc: 0.955, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 57, loss: 0.154, acc: 0.957, val_loss: 0.111, val_acc: 0.966\n",
      "epoch: 58, loss: 0.154, acc: 0.955, val_loss: 0.11, val_acc: 0.967\n",
      "epoch: 59, loss: 0.152, acc: 0.956, val_loss: 0.109, val_acc: 0.967\n",
      "epoch: 60, loss: 0.15, acc: 0.958, val_loss: 0.109, val_acc: 0.968\n",
      "epoch: 61, loss: 0.146, acc: 0.958, val_loss: 0.108, val_acc: 0.967\n",
      "epoch: 62, loss: 0.147, acc: 0.958, val_loss: 0.106, val_acc: 0.969\n",
      "epoch: 63, loss: 0.145, acc: 0.958, val_loss: 0.107, val_acc: 0.969\n",
      "epoch: 64, loss: 0.143, acc: 0.959, val_loss: 0.106, val_acc: 0.969\n",
      "epoch: 65, loss: 0.138, acc: 0.961, val_loss: 0.104, val_acc: 0.969\n",
      "epoch: 66, loss: 0.141, acc: 0.960, val_loss: 0.106, val_acc: 0.968\n",
      "epoch: 67, loss: 0.14, acc: 0.960, val_loss: 0.103, val_acc: 0.970\n",
      "epoch: 68, loss: 0.139, acc: 0.959, val_loss: 0.104, val_acc: 0.969\n",
      "epoch: 69, loss: 0.136, acc: 0.962, val_loss: 0.103, val_acc: 0.970\n",
      "epoch: 70, loss: 0.137, acc: 0.961, val_loss: 0.101, val_acc: 0.970\n",
      "epoch: 71, loss: 0.138, acc: 0.961, val_loss: 0.102, val_acc: 0.970\n",
      "epoch: 72, loss: 0.132, acc: 0.963, val_loss: 0.101, val_acc: 0.970\n",
      "epoch: 73, loss: 0.132, acc: 0.963, val_loss: 0.102, val_acc: 0.970\n",
      "epoch: 74, loss: 0.129, acc: 0.963, val_loss: 0.101, val_acc: 0.971\n",
      "epoch: 75, loss: 0.127, acc: 0.963, val_loss: 0.0988, val_acc: 0.972\n",
      "epoch: 76, loss: 0.128, acc: 0.963, val_loss: 0.0996, val_acc: 0.972\n",
      "epoch: 77, loss: 0.126, acc: 0.964, val_loss: 0.0986, val_acc: 0.972\n",
      "epoch: 78, loss: 0.123, acc: 0.965, val_loss: 0.0973, val_acc: 0.972\n",
      "epoch: 79, loss: 0.123, acc: 0.965, val_loss: 0.0983, val_acc: 0.972\n",
      "epoch: 80, loss: 0.126, acc: 0.964, val_loss: 0.0995, val_acc: 0.971\n",
      "epoch: 81, loss: 0.122, acc: 0.965, val_loss: 0.0991, val_acc: 0.972\n",
      "epoch: 82, loss: 0.119, acc: 0.966, val_loss: 0.0964, val_acc: 0.973\n",
      "epoch: 83, loss: 0.119, acc: 0.966, val_loss: 0.0981, val_acc: 0.971\n",
      "epoch: 84, loss: 0.117, acc: 0.966, val_loss: 0.0977, val_acc: 0.972\n",
      "epoch: 85, loss: 0.119, acc: 0.965, val_loss: 0.0979, val_acc: 0.973\n",
      "epoch: 86, loss: 0.117, acc: 0.966, val_loss: 0.0971, val_acc: 0.972\n",
      "epoch: 87, loss: 0.115, acc: 0.967, val_loss: 0.0953, val_acc: 0.973\n",
      "epoch: 88, loss: 0.115, acc: 0.967, val_loss: 0.0966, val_acc: 0.972\n",
      "epoch: 89, loss: 0.114, acc: 0.966, val_loss: 0.0959, val_acc: 0.973\n",
      "epoch: 90, loss: 0.113, acc: 0.967, val_loss: 0.0949, val_acc: 0.973\n",
      "epoch: 91, loss: 0.112, acc: 0.968, val_loss: 0.0964, val_acc: 0.973\n",
      "epoch: 92, loss: 0.111, acc: 0.968, val_loss: 0.0948, val_acc: 0.973\n",
      "epoch: 93, loss: 0.106, acc: 0.970, val_loss: 0.0958, val_acc: 0.974\n",
      "epoch: 94, loss: 0.108, acc: 0.969, val_loss: 0.0935, val_acc: 0.974\n",
      "epoch: 95, loss: 0.109, acc: 0.968, val_loss: 0.0943, val_acc: 0.973\n",
      "epoch: 96, loss: 0.108, acc: 0.969, val_loss: 0.0951, val_acc: 0.973\n",
      "epoch: 97, loss: 0.106, acc: 0.969, val_loss: 0.0955, val_acc: 0.972\n",
      "epoch: 98, loss: 0.104, acc: 0.969, val_loss: 0.0943, val_acc: 0.974\n",
      "epoch: 99, loss: 0.103, acc: 0.970, val_loss: 0.094, val_acc: 0.974\n",
      "epoch: 100, loss: 0.103, acc: 0.970, val_loss: 0.0925, val_acc: 0.974\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEECAYAAADOJIhPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAbhklEQVR4nO3dfXRddb3n8ff3nDw2D03SpqQtbToiNbS21EVBS7krl+KFK9XlYmaWo4OAgy68DwjcuaKMi7nXC1wul+mdUbx4lcG1ZnxaddRRdApFVmcAkSKkYJW20kpraWlpUvL8dE6S850/zsnhNCZtWrKz0/w+r7Wykr3PTs731+zuT36/vfdvm7sjIiJhSsRdgIiIxEchICISMIWAiEjAFAIiIgFTCIiIBKwo7gJOx/z5833ZsmVxlyEiclbZsWPHcXevH++1syoEli1bRktLS9xliIicVczs4ESvaThIRCRgCgERkYApBEREAqYQEBEJmEJARCRgCgERkYApBEREAhZECKRSKbZv3x53GSIiM04wIXDllVfS29sbdykiIjNKECFQXV3NJZdcwrZt2+IuRURkRgkiBAA2btzIli1b4i5DRGRGCSoEHn30UfQ4TRGRtwQTAsuXL6esrIydO3fGXYqIyIwRTAiYmYaERETGCCYEQOcFRETGCioEmpub2bVrF8ePH4+7FBGRGSGoECgtLWXDhg1s3bo17lJERGaEoEIANCQkIlIouBC4+uqr2bp1qy4VFREhwBBYtGgRfX19pNPpuEsREYldcCEAUFFRQV9fX9xliIjETiEgIhIwhYCISMAUAiIiAVMIiIgETCEgIhKwYENATxkTEQk0BCorK9UTEBEh0BDQcJCISJZCQEQkYAoBEZGAFUXxQ83sPOAe4EXgXOBNd79rzDZlwCbgdeB84D533xtFPWNVVFRw7Nix6XgrEZEZLZIQAOqAze7+CICZ7TazLe6+o2Cb24DX3P1+M1sFfAP4o4jqOYF6AiIiWZEMB7n7C6MBUPA+Y4+6G4Htue1/A1xoZtVjf5aZ3WRmLWbW0tbWNiX1KQRERLIiPydgZtcAj7v7b8e8tADoKVjuzq07gbs/5O5r3X1tfX39lNSkEBARyYpqOAgAM7scuJzs0M9YrUBVwXJ1bl3kFAIiIlmR9QTMbCNwFXAr0GBm68ysrmDIZwuwLrftKmCnu3dHVU8h3TEsIpIV1dVBFwHfA1qA/wdUAA8C1wDtwH3Al4FNZnYn8E7gk1HUMh7dMSwikhVJCOSuAqo8xTYDwF9G8f6nouEgEZEs3SwmIhIwhYCISMAUAiIiAQsyBEpKSgBIp9MxVyIiEq8gQwDUGxARAYVA3GWIiMQq6BDQDWMiErqgQ0A9AREJXbAhoLuGRUQCDgH1BEREFAJxlyEiEiuFgIhIwBQCIiIBUwiIiARMISAiErCgQ0A3i4lI6IIOAfUERCR0CgERkYAFGwK6Y1hEJOAQUE9AREQhEHcZIiKxUgiIiARMISAiEjCFgIhIwBQCIiIBCzoEdMewiIQu2BAoKSnB3Umn03GXIiISm2BDwMw0JCQiwQs2BEB3DYuIBB0C6gmISOgUAgoBEQmYQkAhICIBUwgoBEQkYAoBhYCIBCz4ENANYyISsuBDQD0BEQmZQkAhICIBiyQEzKzBzB42sxcmeP2PzexXZvZk7uP2KOo4FYWAiISuKKKfexnwCLDmJNvc5u5PRvT+k1JZWUlbW1ucJYiIxCqSnoC7/wDoOcVm15nZZ83sLjNbMtFGZnaTmbWYWctUH7DVExCR0MV1TmA3cLe7bwK+BzxhZuPW4u4Puftad19bX18/pUUoBEQkdFENB52Uu7cWfL3LzGqAJcDB6axDISAioZu2noCZVZhZfe7rO8ysLvd1HVACHJuuWkYpBEQkdJH0BMysGbgOWGhmdwL/BHwCWAX8GfB74MtmthtYAVzn7oNR1HIyCgERCV0kIeDuTwFPjVn9YMHrm4HNUbz36dAdwyISOt0spp6AiARMIaAQEJGAKQQUAiISsKBDQM8YFpHQBR0CJSUlZDIZhoaG4i5FRCQWQYeAmekKIREJWtAhAFBTU0NnZ2fcZYiIxCL4EKitraWjoyPuMkREYqEQUAiISMCCD4G6ujqFgIgEK/gQqK2tpb29Pe4yRERioRDQcJCIBEwhoBAQkYApBBQCIhIwhYBCQEQCFnwI6OogEQnZKUPAzC41s3eaWaOZfcnMLpqOwqaLegIiErLJ9ARuANqB/wrsBT4daUXTTJeIikjIJhMC+4EBYIG7fxX4XbQlTS/1BEQkZJMJgRXA94Afm9mi3PKsUVNTQ3d3N5lMJu5SRESm3WQeNP/XwHrg/wDvBh6OtKJplkwmqayspKuri9ra2rjLERGZVpPpCSwHdgHnAv+B7NDQrKIrhEQkVMGfGAadFxCRcAV/Yhh0hZCIhOt0Tgw/MhtPDIN6AiISrtM5MfxTYDWz7MQwKAREJFyT6Qm8CSwAvgxcBPwi0opioBAQkVBNJgT+G/Ae4ACwNrc8q+jqIBEJ1WSGg9rc/e9HF8zsi9GVE4/a2lr2798fdxkiItNuMj2BuadYPutpOEhEQjWZnsA+M9tJdjjoXwFfibak6adLREUkVKcMAXf/72b2c2Al8DLQHHlV00w9AREJ1YQhYGbtQGfhqtznauChKIuabgoBEQnVyXoCN7v7d8euNLN/H2E9sVAIiEioJjwxPF4AnGz92Wzu3Ln09vYyMjISdykiItMq+GcMQ3Y66aqqKrq6uuIuRURkWikEcnSFkIiEaDKXiJ42M2sA7gEudPeLx3k9AdwL9AKNwDfc/bkoapksnRcQkRBFEgLAZcAjwJoJXv8IUO3ud5hZHfCcmV3g7rENyisERCREkQwHufsPgJ6TbLIR2J7bth0YJHsfQmwUAiISorjOCSzgxJDozq37A2Z2k5m1mFlLW1tbZAVpEjkRCVFcIdAKVBUsV+fW/QF3f8jd17r72vr6+sgKUk9AREI0bSFgZhVmNnoU3wKsy62vA8rIPsw+NgoBEQlRJCFgZs3AdcBCM7vTzMqBTwB35zb5X0CPmf0t8F+A6+M8KQy6RFREwhTJ1UHu/hTw1JjVDxa8ngE+H8V7nyn1BEQkRLpZLEchICIhUgjk6OogEQmRQiBHPQERCZFCIEchICIhUgjkVFdX09fXx/DwcNyliIhMG4VATiKRoLq6ms7OzlNvLCIySygECtTX19PaOu6NyyIis5JCoEBTUxN79uyJuwwRkWmjECiwcuVKdu2KdfYKEZFppRAooBAQkdAoBAooBEQkNAqBAk1NTbz66qsMDQ3FXYqIyLRQCBQoKytjyZIl7Nu3L+5SRESmhUJgDA0JiUhIFAJjKAREJCQKgTFWrlzJyy+/HHcZIiLTQiEwhnoCIhIShcAY73rXuzhw4ACpVCruUkREIqcQGKO0tJRly5axd+/euEsREYmcQmAcGhISkVAoBMahEBCRUCgExqEQEJFQKATGoRAQkVAoBMaxfPlyDh48yODgYNyliIhESiEwjpKSEs477zw9YEZEZj2FwAQuvfRSnn766bjLEBGJlEJgAldccQXbtm2LuwwRkUgpBCawYcMGnn76aYaHh+MuRUQkMgqBCSxYsIClS5fS0tISdykiIpFRCJzEhg0bNCQkIrOaQuAkdF5ARGY7hcBJNDc38/zzzzMwMBB3KSIikVAInER1dTWrVq3i2WefjbsUEZFIKAROQUNCIjKbKQROQSEgIrOZQuAU1q1bx+7du+ns7Iy7FBGRKacQOIWysjLWr1/PY489FncpIiJTLrIQMLP3m9lXzeyLZva347z+CTN7zsyezH1cF1Utb9dNN93EV77ylbjLEBGZcpGEgJnNAb4G/JW7fxFYbWZXjLPpR939j3Mf34qilqnw4Q9/mKNHj/LLX/4y7lJERKZUVD2BdcBBd0/lln8BbBxnu5vN7LNm9jdmVhdRLW9bMpnklltu4Utf+lLcpYiITKmoQmAB0FOw3J1bV+gp4B/dfRPQAnx/vB9kZjeZWYuZtbS1tUVS7GTceOONPP744xw+fDi2GkREplpUIdAKVBUsV+fW5bn7AXcfPar/X6DZzJJjf5C7P+Tua919bX19fUTlntrcuXO57rrrePDBB2OrQURkqkUVAtuBRjMrzS2vB7aYWZ2ZVQOY2T+YWVHu9fOBA+4+ElE9U+KWW27h4Ycfpr+/P+5SRESmRCQh4O79wJ8DD5jZPcCv3X0bcAfwF7nN3gD+xcy+AHwBmLFXB40677zzuOyyy9QbEJFZw9w97hombe3atR73/P779u1j3bp1vPjiiyxdujTWWkREJsPMdrj72vFe081ip+n888/n1ltv5eabb+ZsClARkfEoBM7A5z73Ofbu3cuPf/zjuEsREXlbFAJnoLS0lK9//evccsst9PT0nPobRERmKIXAGWpubuaqq67ihhtuYGhoKO5yRETOiELgbXjwwQdJpVJce+21DA8Px12OiMhpUwi8DaWlpfzwhz+ku7ub66+/npGRGX2bg4jIH1AIvE1lZWX86Ec/4vjx43z0ox9lcHAw7pJERCZNITAFysvL+clPfkJRURFXXHEFx48fj7skEZFJUQhMkbKyMr7zne/Q3NzMunXr2LdvX9wliYickkJgCiUSCe69914+//nPc+mll/Ld73437pJERE5KIRCBT33qUzzxxBPcdddd3HDDDbqXQERmLIVARNasWcOOHTsoLi6mqamJTZs2KQxEZMZRCESooqKChx9+mC1bttDS0sI73vEO7r77bvr6+uIuTUQEUAhMizVr1rB582aeffZZdu3aRVNTE9/+9rfJZDJxlyYigVMITKPzzz+fzZs3s3nzZh544AHWrl3L1772NTo7O+MuTUQCpRCIwfr163nuuee455572LZtG42NjXz84x9n586dcZcmIoFRCMQkkUhw9dVX8/3vf5/9+/dz4YUX8oEPfIAPfvCDPPPMM3pWgYhMC4XADDBv3jxuv/129u/fz4c+9CFuvPFGmpqauPvuu9m/f3/c5YnILKYQmEHKysr49Kc/zSuvvMI3v/lNWltbed/73seKFSu47bbbePTRR/WQexGZUnrG8AyXyWR46aWX+NnPfsbWrVt56aWXuOyyy7j66qtpbm5mxYoVJJPJuMsUkRnsZM8YVgicZTo7O3niiSd47LHHeOaZZ2htbeW9730vl19+OVdeeSVr1qwhkVAHT0TeohCYxdra2ti+fTvbtm3j8ccfp729nZUrV7J48WIWLVrEmjVreP/738+CBQviLlVEYqIQCMihQ4fYu3cvr7/+OocPH+b555/nySefpLGxkXXr1tHU1MQFF1zAihUrOPfcczGzuEsWkYidLASKprsYidaSJUtYsmTJCeuGh4d54YUXePHFF9mzZw8//elP2b17N4ODg6xatYpVq1axevVqVq1axYoVK6ipqYmpehGZbuoJBKytrY3f/OY3/PrXv85/3rNnD8lkksbGRpYuXZofVlq0aBGLFy9m8eLFLFmyhLq6urjLF5FJ0nCQTJq7097ezsGDBzl06BBHjhzhyJEjvP766xw5coTDhw/z2muvUVJSQlNTE8uXL2fx4sU0NDRwzjnnUF1dTWVlJVVVVSxcuJB58+ZpyEkkZgoBmVLuzrFjx3jllVfYu3cvR48e5Y033uDYsWP09PTQ29tLd3c3R44cYXBwkHPPPZclS5bkP1dUVJBIJEgkEtTW1uZ7Gg0NDcyfP5+iIo1SikwlnROQKWVmNDQ00NDQQHNz80m37evr49ChQxw6dIjDhw9z6NAhuru7GRkZYWRkhD179uR7GseOHaO9vZ3q6moWLVpEY2MjjY2NNDQ0UFFRwZw5c6ioqKCyspKKigqqqqqoqamhtraWqqoqSkpKKC4uVs9D5DQoBCRSFRUVNDU10dTUNKntR0ZG6Ojo4MiRI/z+97/n4MGDtLa2cuTIEfr6+vIfo72Nrq4uOjo66O7uZmhoiOHhYUpLS6mvr6ehoYH6+npKS0spKiqipKSE2tpa6urqqKuro7y8nJKSEkpLS5k7dy51dXXU1tZSVlZGMpnMf09paWn+ZyhgZLZRCMiMkkwmmT9/PvPnz2f16tWn/f3uzuDgIK2trRw7dozW1lbS6TTDw8OkUik6Oztpb2/nwIEDDAwMkE6nGRwcpLu7m/b2djo6OkilUgwPDzM8PEw6nSaVSpFKpUgkEsydO5e5c+dSVlZ2wiR/yWSSZDJJcXExc+bMYc6cOVRWVuYDp6amhqqqKiorKykrK6O/v5++vj5SqRTl5eX5Xk5paekJwVNWVpYPoNFgKioqori4+ITtFE5yphQCMquYGeXl5fmhpKk0ODhIV1cXXV1dpNPp/Pu5OyMjI2QyGdLpNP39/fT399PT00NHRwft7e20trayf/9+ent7GRwcPOGgPzg4mO/hFIZOKpVicHCQwcHB/PDZyMhIPpxGP1KpFKWlpVRVVTF37lxqampIJBIMDAwwMDCAu+cDZc6cOVRVVVFdXU1RURFdXV10dnaSTqeZN28e8+fPp7q6muHhYYaGhshkMieEVGVlZX44bmRkhHQ6zdDQEMXFxfnAGl2fTqdJJpP5sKqsrMyHaDKZzG8D2XmzRj/mzJlDeXk5yWQy/+8w2paBgQGGhoZIJBIkk0lKS0uZN28e8+bNo7i4GHdnYGCAvr4+zCy/nbuTyWRw93yAlpSU6O56FAIikzZ6kDrnnHPiLuUEmUyGVCpFT09PfnjM3SkvL6e8vBwzyx9M+/r66Onpobu7m+HhYWpqaqipqaG4uJj29naOHz9OV1dXvreRSCROCKm2tjYOHDhAX19ffrisqKiI4eHhfGCNHpyLi4vzgZBKpejt7c2HaCaTyR+I3T0feKMH+v7+fkZGRk7oDY2GQ0lJST4QU6kUb775Ju3t7ZSUlDA4OJjfFshvl0gkMDPMLN8rTKfTlJeXU11dTVVVFZlMhv7+/nxwFobFaI/L3ent7aWvr49MJpO/Eq64uDj/b+zu1NbW5s9VjbZ/NPSGhoZIp9P52jKZDGVlZfnfVzKZzF84UV5eTkVFBRUVFdx6662sXLlyyvcfhYDIWW70YFFeXh7s9CCjB/DRg+hkuDv9/f10d3fT09NDMpk8IThHD9aFvTMzy/eEEokEvb299Pb2kk6n84EF0NHRQUdHBz09PfkQGQ2U0QsYRocQR0N6NAAzmQyZTIaRkZF8r6avr4+qqqpI/u0UAiJy1kskElRWVp7W95hZ/q/shQsXntH71tfXn9H3zSQaEBMRCZhCQEQkYAoBEZGAKQRERAIW2YlhM3s/8K+BVsDd/e/GvF4GbAJeB84H7nP3vVHVIyIifyiSEDCzOcDXgJXunjKzH5rZFe6+rWCz24DX3P1+M1sFfAP4oyjqERGR8UU1HLQOOOjuqdzyL4CNY7bZCGwHcPffABeaWfXYH2RmN5lZi5m1tLW1RVSuiEiYogqBBUBPwXJ3bt3pboO7P+Tua9197Wy4JldEZCaJ6pxAK1B4e1t1bt3pbnOCHTt2HDezg2dY03zg+Bl+79ksxHaH2GYIs90hthlOv90TTqQVVQhsBxrNrDQ3JLQe+KqZ1QHD7t4NbCE7bPTz3DmBnbn1E3L3M+4KmFnLRA9VmM1CbHeIbYYw2x1im2Fq2x1JCLh7v5n9OfCAmbUBv3b3bWZ2P9AO3Ad8GdhkZncC7wQ+GUUtIiIyscguEXX3J4Anxqz7XMHXA8BfRvX+IiJyaiHdLPZQ3AXEJMR2h9hmCLPdIbYZprDdZ9WD5kVEZGqF1BMQEZExFAIiIgEL4qEyp5rHaDYws/OAe4AXgXOBN939rtxlufcB+8nO0fQFdz8WX6VTz8zKgV8CP3P3z4YwL5WZvQv4GDAANANfJLt//2fgd8Ay4K/dvTemEiNhZreTbdtxsr/bTwLlzLJ93MwayP5/vtDdL86tm3C/NrOPA+8BRoBX3f3rk34zd5/VH8Acsv8pSnPLPwSuiLuuCNp5MfDhguXdwEVk53D6SG7dh4BvxV1rBG3/J+B/Aptyy3cAn8t9vQr4edw1TnF7k2Tvs0nklhcC9cBW4JLcus8Ad8dd6xS3u4HsJeaj7X4EuHY27uPAv821paVg3bj7Ndk/+n7FW+d4XwDOn+x7hTAcNJl5jM567v6Cuz9SsCoB9FEwRxOzsO1mdh3Zdh0oWD2peanOYhcDBnzGzP4T2YNFJ3A52QMAzMLfNdAPpMnOLgBQCexiFu7j7v4DTpxWByber68CdnguAXLbfGCy7xXCcNCk5iiaTczsGuBxd/+tmRW2vxuoNbMidx+Or8KpYWYrgAvc/QtmtrrgpYl+5ye9I/0s0kj2j5uPuXuXmX0bmAcMFBwIZt1+7u7dueGg75nZUeAw2V7+rN3Hx5hov35bx7gQegKnPUfR2czMLif7F+Ff5VYVtr8a6JhF/zmuAQbN7A7gMuASM7uN2f877wZ+6+5dueVngHcD5WZmuXWzrc2Y2RrgdmCju3+C7HmBv2F27+OFJtqv39b+HkJPYNx5jGKuKRJmtpHsMxluBRaaWSNvzdF0iGzbt8RX4dRy978f/Tp30qzS3b+U+/q05qU6y/wSmGdmSXcfIdsz2EW2N3Ax8Dyz7HedsxhoLzjAHwWWMov38THGnW/NzB4nOzRouZ7gOuArk/2hQdwsZmZ/QvZESxsw5LPz6qCLgKeAltyqCuBB4CfAPwIHgfOAO/wsv3JiLDP7N2SnICkh2+Yfk72K4ijZeanu9dl3ddA1wAay+/RSsieCzyH7l/H+3Lr/6LPo6iAzSwIPAINkz4G8m+zDqVLMsn3czJqB64E/Bf6F7MUPMMF+nbs6aC3Zq4P2+mlcHRRECIiIyPhCOCcgIiITUAiIiARMISAiEjCFgIhIwBQCIiIBUwiIRMjMNprZATNbFnctIuNRCIhEyN23kL1+XWRGCuGOYZFTMrO7yP5/GCE7D8sbZG9MupfsLfkXAre6+wEzWw/cQHbemibgTnc/klv/CWAv2Tt3N7n787m3+IiZvQO4APhQ7k7Pv8u9Zwoocfc7p6e1Im9RCEjwzOwq4H3ufmVu+Umyd6J2Av/b3X9nZv8OuN/MPgJ8D3iPu7fl1m8ys2tz6y9y92Nm9m6yd22Pesnd7zezfwb+hOyU5jcBG9x9j5ldOk3NFTmBQkAEVgNzchPRQXYOmvrc1/tzn38HrATmA9Xu3law/sKC9ccA3P3lMe/xu9zn47w12dfHgHvN7ByyvY5np6xFIpOkEBCBncA6d78PwMw28NZB+x25r5eTfVDPcaDLzBa4eyvZJzz9auz63NTWle4+emAfb36WKne/Jjfd905gc0TtE5mQ5g4SAczsTrLDN8NAGdmnOL1K9rGFS8g+uu8z7v5qbuz/xtzr7yI7YdnRgvX7gEXAncB7gYeAbwH/A3gY6AD+jOwTsV4k+3jEfne/d1oaK1JAISAyATP7vbsvi7sOkSjpElGRceRO9M41s7+IuxaRKKknICISMPUEREQCphAQEQmYQkBEJGAKARGRgCkEREQC9v8BAMJXVSO2eBUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.086, test_acc: 0.976\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        self.l3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.a3 = nn.ReLU()\n",
    "        self.d3 = nn.Dropout(0.5)\n",
    "        self.l4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.layers = [self.l1, self.a1, self.d1,\n",
    "                       self.l2, self.a2, self.d2,\n",
    "                       self.l3, self.a3, self.d3,\n",
    "                       self.l4]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(123)\n",
    "    torch.manual_seed(123)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    '''\n",
    "    1. データの準備\n",
    "    '''\n",
    "    root = os.path.join('~', '.torch', 'mnist')\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    lambda x: x.view(-1)])\n",
    "    mnist_train = datasets.MNIST(root=root,\n",
    "                                 download=True,\n",
    "                                 train=True,\n",
    "                                 transform=transform)\n",
    "    mnist_test = datasets.MNIST(root=root,\n",
    "                                download=True,\n",
    "                                train=False,\n",
    "                                transform=transform)\n",
    "\n",
    "    n_samples = len(mnist_train)\n",
    "    n_train = int(n_samples * 0.8)\n",
    "    n_val = n_samples - n_train\n",
    "\n",
    "    mnist_train, mnist_val = \\\n",
    "        random_split(mnist_train, [n_train, n_val])\n",
    "\n",
    "    train_dataloader = DataLoader(mnist_train,\n",
    "                                  batch_size=100,\n",
    "                                  shuffle=True)\n",
    "    val_dataloader = DataLoader(mnist_val,\n",
    "                                batch_size=100,\n",
    "                                shuffle=False)\n",
    "    test_dataloader = DataLoader(mnist_test,\n",
    "                                 batch_size=100,\n",
    "                                 shuffle=False)\n",
    "\n",
    "    '''\n",
    "    2. モデルの構築\n",
    "    '''\n",
    "    model = DNN(784, 200, 10).to(device)\n",
    "\n",
    "    '''\n",
    "    3. モデルの学習\n",
    "    '''\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizers.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    def compute_loss(t, y):\n",
    "        return criterion(y, t)\n",
    "\n",
    "    def train_step(x, t):\n",
    "        model.train()\n",
    "        preds = model(x)\n",
    "        loss = compute_loss(t, preds)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss, preds\n",
    "\n",
    "    def val_step(x, t):\n",
    "        model.eval()\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, t)\n",
    "\n",
    "        return loss, preds\n",
    "\n",
    "    epochs = 100\n",
    "    hist = {'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        val_loss = 0.\n",
    "        val_acc = 0.\n",
    "\n",
    "        for (x, t) in train_dataloader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            loss, preds = train_step(x, t)\n",
    "            train_loss += loss.item()\n",
    "            train_acc += \\\n",
    "                accuracy_score(t.tolist(),\n",
    "                               preds.argmax(dim=-1).tolist())\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc /= len(train_dataloader)\n",
    "\n",
    "        for (x, t) in val_dataloader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            loss, preds = val_step(x, t)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += \\\n",
    "                accuracy_score(t.tolist(),\n",
    "                               preds.argmax(dim=-1).tolist())\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_acc /= len(val_dataloader)\n",
    "\n",
    "        hist['val_loss'].append(val_loss)\n",
    "        hist['val_accuracy'].append(val_acc)\n",
    "\n",
    "        print('epoch: {}, loss: {:.3}, acc: {:.3f}'\n",
    "              ', val_loss: {:.3}, val_acc: {:.3f}'.format(\n",
    "                  epoch+1,\n",
    "                  train_loss,\n",
    "                  train_acc,\n",
    "                  val_loss,\n",
    "                  val_acc\n",
    "              ))\n",
    "\n",
    "    '''\n",
    "    4. モデルの評価\n",
    "    '''\n",
    "    # 検証データの誤差の可視化\n",
    "    val_loss = hist['val_loss']\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.rc('font', family='serif')\n",
    "    plt.plot(range(len(val_loss)), val_loss,\n",
    "             color='black', linewidth=1)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    # plt.savefig('output.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    # 正解率を可視化する場合\n",
    "    # val_acc = hist['val_accuracy']\n",
    "    #\n",
    "    # fig = plt.figure()\n",
    "    # plt.rc('font', family='serif')\n",
    "    # plt.plot(range(len(val_acc)), val_acc,\n",
    "    #          color='black', linewidth=1)\n",
    "    # plt.xlabel('epochs')\n",
    "    # plt.ylabel('acc')\n",
    "    # plt.savefig('output_acc.jpg')\n",
    "    # plt.show()\n",
    "\n",
    "    # テストデータの評価\n",
    "    def test_step(x, t):\n",
    "        return val_step(x, t)\n",
    "\n",
    "    test_loss = 0.\n",
    "    test_acc = 0.\n",
    "\n",
    "    for (x, t) in test_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, preds = test_step(x, t)\n",
    "        test_loss += loss.item()\n",
    "        test_acc += \\\n",
    "            accuracy_score(t.tolist(),\n",
    "                           preds.argmax(dim=-1).tolist())\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "    print('test_loss: {:.3f}, test_acc: {:.3f}'.format(\n",
    "        test_loss,\n",
    "        test_acc\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
