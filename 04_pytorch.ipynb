{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        self.l3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.a3 = nn.ReLU()\n",
    "        self.d3 = nn.Dropout(0.5)\n",
    "        self.l4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.layers = [self.l1, self.a1, self.d1,\n",
    "                       self.l2, self.a2, self.d2,\n",
    "                       self.l3, self.a3, self.d3,\n",
    "                       self.l4]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 2.29, acc: 0.138, val_loss: 2.28, val_acc: 0.213\n",
      "epoch: 2, loss: 2.25, acc: 0.265, val_loss: 2.16, val_acc: 0.482\n",
      "epoch: 3, loss: 1.94, acc: 0.410, val_loss: 1.38, val_acc: 0.633\n",
      "epoch: 4, loss: 1.3, acc: 0.553, val_loss: 0.871, val_acc: 0.727\n",
      "epoch: 5, loss: 1.0, acc: 0.661, val_loss: 0.669, val_acc: 0.805\n",
      "epoch: 6, loss: 0.826, acc: 0.731, val_loss: 0.539, val_acc: 0.845\n",
      "epoch: 7, loss: 0.708, acc: 0.777, val_loss: 0.461, val_acc: 0.868\n",
      "epoch: 8, loss: 0.626, acc: 0.807, val_loss: 0.407, val_acc: 0.885\n",
      "epoch: 9, loss: 0.567, acc: 0.830, val_loss: 0.372, val_acc: 0.894\n",
      "epoch: 10, loss: 0.523, acc: 0.845, val_loss: 0.343, val_acc: 0.901\n",
      "epoch: 11, loss: 0.486, acc: 0.856, val_loss: 0.318, val_acc: 0.908\n",
      "epoch: 12, loss: 0.456, acc: 0.866, val_loss: 0.302, val_acc: 0.914\n",
      "epoch: 13, loss: 0.429, acc: 0.876, val_loss: 0.283, val_acc: 0.919\n",
      "epoch: 14, loss: 0.408, acc: 0.883, val_loss: 0.27, val_acc: 0.921\n",
      "epoch: 15, loss: 0.389, acc: 0.890, val_loss: 0.254, val_acc: 0.926\n",
      "epoch: 16, loss: 0.37, acc: 0.894, val_loss: 0.242, val_acc: 0.929\n",
      "epoch: 17, loss: 0.358, acc: 0.898, val_loss: 0.232, val_acc: 0.932\n",
      "epoch: 18, loss: 0.344, acc: 0.902, val_loss: 0.222, val_acc: 0.934\n",
      "epoch: 19, loss: 0.328, acc: 0.908, val_loss: 0.215, val_acc: 0.936\n",
      "epoch: 20, loss: 0.317, acc: 0.910, val_loss: 0.205, val_acc: 0.939\n",
      "epoch: 21, loss: 0.307, acc: 0.913, val_loss: 0.202, val_acc: 0.941\n",
      "epoch: 22, loss: 0.297, acc: 0.918, val_loss: 0.193, val_acc: 0.943\n",
      "epoch: 23, loss: 0.285, acc: 0.920, val_loss: 0.187, val_acc: 0.945\n",
      "epoch: 24, loss: 0.276, acc: 0.922, val_loss: 0.181, val_acc: 0.946\n",
      "epoch: 25, loss: 0.274, acc: 0.923, val_loss: 0.175, val_acc: 0.947\n",
      "epoch: 26, loss: 0.263, acc: 0.925, val_loss: 0.171, val_acc: 0.949\n",
      "epoch: 27, loss: 0.259, acc: 0.927, val_loss: 0.168, val_acc: 0.949\n",
      "epoch: 28, loss: 0.25, acc: 0.931, val_loss: 0.164, val_acc: 0.951\n",
      "epoch: 29, loss: 0.247, acc: 0.931, val_loss: 0.159, val_acc: 0.952\n",
      "epoch: 30, loss: 0.24, acc: 0.933, val_loss: 0.156, val_acc: 0.952\n",
      "epoch: 31, loss: 0.236, acc: 0.935, val_loss: 0.154, val_acc: 0.954\n",
      "epoch: 32, loss: 0.23, acc: 0.936, val_loss: 0.15, val_acc: 0.955\n",
      "epoch: 33, loss: 0.225, acc: 0.936, val_loss: 0.147, val_acc: 0.956\n",
      "epoch: 34, loss: 0.221, acc: 0.937, val_loss: 0.146, val_acc: 0.957\n",
      "epoch: 35, loss: 0.213, acc: 0.940, val_loss: 0.143, val_acc: 0.957\n",
      "epoch: 36, loss: 0.21, acc: 0.940, val_loss: 0.14, val_acc: 0.958\n",
      "epoch: 37, loss: 0.211, acc: 0.941, val_loss: 0.136, val_acc: 0.959\n",
      "epoch: 38, loss: 0.204, acc: 0.943, val_loss: 0.134, val_acc: 0.959\n",
      "epoch: 39, loss: 0.201, acc: 0.943, val_loss: 0.133, val_acc: 0.960\n",
      "epoch: 40, loss: 0.2, acc: 0.944, val_loss: 0.131, val_acc: 0.960\n",
      "epoch: 41, loss: 0.192, acc: 0.947, val_loss: 0.13, val_acc: 0.960\n",
      "epoch: 42, loss: 0.194, acc: 0.945, val_loss: 0.127, val_acc: 0.961\n",
      "epoch: 43, loss: 0.187, acc: 0.947, val_loss: 0.126, val_acc: 0.961\n",
      "epoch: 44, loss: 0.186, acc: 0.948, val_loss: 0.124, val_acc: 0.962\n",
      "epoch: 45, loss: 0.183, acc: 0.948, val_loss: 0.123, val_acc: 0.962\n",
      "epoch: 46, loss: 0.18, acc: 0.949, val_loss: 0.121, val_acc: 0.963\n",
      "epoch: 47, loss: 0.178, acc: 0.950, val_loss: 0.12, val_acc: 0.963\n",
      "epoch: 48, loss: 0.175, acc: 0.951, val_loss: 0.119, val_acc: 0.964\n",
      "epoch: 49, loss: 0.169, acc: 0.953, val_loss: 0.117, val_acc: 0.964\n",
      "epoch: 50, loss: 0.171, acc: 0.952, val_loss: 0.117, val_acc: 0.965\n",
      "epoch: 51, loss: 0.166, acc: 0.954, val_loss: 0.117, val_acc: 0.965\n",
      "epoch: 52, loss: 0.163, acc: 0.955, val_loss: 0.116, val_acc: 0.965\n",
      "epoch: 53, loss: 0.163, acc: 0.955, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 54, loss: 0.162, acc: 0.954, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 55, loss: 0.16, acc: 0.954, val_loss: 0.113, val_acc: 0.966\n",
      "epoch: 56, loss: 0.157, acc: 0.955, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 57, loss: 0.154, acc: 0.957, val_loss: 0.111, val_acc: 0.966\n",
      "epoch: 58, loss: 0.154, acc: 0.955, val_loss: 0.11, val_acc: 0.967\n",
      "epoch: 59, loss: 0.152, acc: 0.956, val_loss: 0.109, val_acc: 0.967\n",
      "epoch: 60, loss: 0.15, acc: 0.958, val_loss: 0.109, val_acc: 0.968\n",
      "epoch: 61, loss: 0.146, acc: 0.958, val_loss: 0.108, val_acc: 0.967\n",
      "epoch: 62, loss: 0.147, acc: 0.958, val_loss: 0.106, val_acc: 0.969\n",
      "epoch: 63, loss: 0.145, acc: 0.958, val_loss: 0.107, val_acc: 0.969\n",
      "epoch: 64, loss: 0.143, acc: 0.959, val_loss: 0.106, val_acc: 0.969\n",
      "epoch: 65, loss: 0.138, acc: 0.961, val_loss: 0.104, val_acc: 0.969\n",
      "epoch: 66, loss: 0.141, acc: 0.960, val_loss: 0.106, val_acc: 0.968\n",
      "epoch: 67, loss: 0.14, acc: 0.960, val_loss: 0.103, val_acc: 0.970\n",
      "epoch: 68, loss: 0.139, acc: 0.959, val_loss: 0.104, val_acc: 0.969\n",
      "epoch: 69, loss: 0.136, acc: 0.962, val_loss: 0.103, val_acc: 0.970\n",
      "epoch: 70, loss: 0.137, acc: 0.961, val_loss: 0.101, val_acc: 0.970\n",
      "epoch: 71, loss: 0.138, acc: 0.961, val_loss: 0.102, val_acc: 0.970\n",
      "epoch: 72, loss: 0.132, acc: 0.963, val_loss: 0.101, val_acc: 0.970\n",
      "epoch: 73, loss: 0.132, acc: 0.963, val_loss: 0.102, val_acc: 0.970\n",
      "epoch: 74, loss: 0.129, acc: 0.963, val_loss: 0.101, val_acc: 0.971\n",
      "epoch: 75, loss: 0.127, acc: 0.963, val_loss: 0.0988, val_acc: 0.972\n",
      "epoch: 76, loss: 0.128, acc: 0.963, val_loss: 0.0996, val_acc: 0.972\n",
      "epoch: 77, loss: 0.126, acc: 0.964, val_loss: 0.0986, val_acc: 0.972\n",
      "epoch: 78, loss: 0.123, acc: 0.965, val_loss: 0.0973, val_acc: 0.972\n",
      "epoch: 79, loss: 0.123, acc: 0.965, val_loss: 0.0983, val_acc: 0.972\n",
      "epoch: 80, loss: 0.126, acc: 0.964, val_loss: 0.0995, val_acc: 0.971\n",
      "epoch: 81, loss: 0.122, acc: 0.965, val_loss: 0.0991, val_acc: 0.972\n",
      "epoch: 82, loss: 0.119, acc: 0.966, val_loss: 0.0964, val_acc: 0.973\n",
      "epoch: 83, loss: 0.119, acc: 0.966, val_loss: 0.0981, val_acc: 0.971\n",
      "epoch: 84, loss: 0.117, acc: 0.966, val_loss: 0.0977, val_acc: 0.972\n",
      "epoch: 85, loss: 0.119, acc: 0.965, val_loss: 0.0979, val_acc: 0.973\n",
      "epoch: 86, loss: 0.117, acc: 0.966, val_loss: 0.0971, val_acc: 0.972\n",
      "epoch: 87, loss: 0.115, acc: 0.967, val_loss: 0.0953, val_acc: 0.973\n",
      "epoch: 88, loss: 0.115, acc: 0.967, val_loss: 0.0966, val_acc: 0.972\n",
      "epoch: 89, loss: 0.114, acc: 0.966, val_loss: 0.0959, val_acc: 0.973\n",
      "epoch: 90, loss: 0.113, acc: 0.967, val_loss: 0.0949, val_acc: 0.973\n",
      "epoch: 91, loss: 0.112, acc: 0.968, val_loss: 0.0964, val_acc: 0.973\n",
      "epoch: 92, loss: 0.111, acc: 0.968, val_loss: 0.0948, val_acc: 0.973\n",
      "epoch: 93, loss: 0.106, acc: 0.970, val_loss: 0.0958, val_acc: 0.974\n",
      "epoch: 94, loss: 0.108, acc: 0.969, val_loss: 0.0935, val_acc: 0.974\n",
      "epoch: 95, loss: 0.109, acc: 0.968, val_loss: 0.0943, val_acc: 0.973\n",
      "epoch: 96, loss: 0.108, acc: 0.969, val_loss: 0.0951, val_acc: 0.973\n",
      "epoch: 97, loss: 0.106, acc: 0.969, val_loss: 0.0955, val_acc: 0.972\n",
      "epoch: 98, loss: 0.104, acc: 0.969, val_loss: 0.0943, val_acc: 0.974\n",
      "epoch: 99, loss: 0.103, acc: 0.970, val_loss: 0.094, val_acc: 0.974\n",
      "epoch: 100, loss: 0.103, acc: 0.970, val_loss: 0.0925, val_acc: 0.974\n",
      "epoch: 101, loss: 0.103, acc: 0.969, val_loss: 0.0934, val_acc: 0.974\n",
      "epoch: 102, loss: 0.103, acc: 0.970, val_loss: 0.0928, val_acc: 0.973\n",
      "epoch: 103, loss: 0.102, acc: 0.971, val_loss: 0.0928, val_acc: 0.974\n",
      "epoch: 104, loss: 0.101, acc: 0.971, val_loss: 0.0927, val_acc: 0.975\n",
      "epoch: 105, loss: 0.101, acc: 0.970, val_loss: 0.0923, val_acc: 0.975\n",
      "epoch: 106, loss: 0.0975, acc: 0.972, val_loss: 0.095, val_acc: 0.973\n",
      "epoch: 107, loss: 0.099, acc: 0.971, val_loss: 0.0921, val_acc: 0.975\n",
      "epoch: 108, loss: 0.0962, acc: 0.972, val_loss: 0.0922, val_acc: 0.974\n",
      "epoch: 109, loss: 0.0979, acc: 0.971, val_loss: 0.0912, val_acc: 0.975\n",
      "epoch: 110, loss: 0.0985, acc: 0.972, val_loss: 0.092, val_acc: 0.975\n",
      "epoch: 111, loss: 0.0965, acc: 0.973, val_loss: 0.0925, val_acc: 0.974\n",
      "epoch: 112, loss: 0.0946, acc: 0.972, val_loss: 0.0932, val_acc: 0.974\n",
      "epoch: 113, loss: 0.0926, acc: 0.973, val_loss: 0.0911, val_acc: 0.975\n",
      "epoch: 114, loss: 0.0951, acc: 0.972, val_loss: 0.0924, val_acc: 0.973\n",
      "epoch: 115, loss: 0.0955, acc: 0.972, val_loss: 0.0917, val_acc: 0.974\n",
      "epoch: 116, loss: 0.0973, acc: 0.971, val_loss: 0.0915, val_acc: 0.975\n",
      "epoch: 117, loss: 0.0937, acc: 0.973, val_loss: 0.0906, val_acc: 0.975\n",
      "epoch: 118, loss: 0.0896, acc: 0.974, val_loss: 0.0923, val_acc: 0.974\n",
      "epoch: 119, loss: 0.0915, acc: 0.974, val_loss: 0.0926, val_acc: 0.974\n",
      "epoch: 120, loss: 0.089, acc: 0.974, val_loss: 0.0905, val_acc: 0.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 121, loss: 0.0917, acc: 0.973, val_loss: 0.0918, val_acc: 0.975\n",
      "epoch: 122, loss: 0.0898, acc: 0.973, val_loss: 0.0896, val_acc: 0.975\n",
      "epoch: 123, loss: 0.0899, acc: 0.973, val_loss: 0.0912, val_acc: 0.975\n",
      "epoch: 124, loss: 0.0877, acc: 0.974, val_loss: 0.0917, val_acc: 0.974\n",
      "epoch: 125, loss: 0.0903, acc: 0.974, val_loss: 0.0919, val_acc: 0.975\n",
      "epoch: 126, loss: 0.0883, acc: 0.975, val_loss: 0.0921, val_acc: 0.975\n",
      "epoch: 127, loss: 0.09, acc: 0.974, val_loss: 0.0914, val_acc: 0.975\n",
      "epoch: 128, loss: 0.0833, acc: 0.976, val_loss: 0.0933, val_acc: 0.975\n",
      "early stopping\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(123)\n",
    "    torch.manual_seed(123)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    '''\n",
    "    1. データの準備\n",
    "    '''\n",
    "    root = os.path.join('~', '.torch', 'mnist')\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    lambda x: x.view(-1)])\n",
    "    mnist_train = datasets.MNIST(root=root,\n",
    "                                 download=True,\n",
    "                                 train=True,\n",
    "                                 transform=transform)\n",
    "    mnist_test = datasets.MNIST(root=root,\n",
    "                                download=True,\n",
    "                                train=False,\n",
    "                                transform=transform)\n",
    "\n",
    "    n_samples = len(mnist_train)\n",
    "    n_train = int(n_samples * 0.8)\n",
    "    n_val = n_samples - n_train\n",
    "\n",
    "    mnist_train, mnist_val = \\\n",
    "        random_split(mnist_train, [n_train, n_val])\n",
    "\n",
    "    train_dataloader = DataLoader(mnist_train,\n",
    "                                  batch_size=100,\n",
    "                                  shuffle=True)\n",
    "    val_dataloader = DataLoader(mnist_val,\n",
    "                                batch_size=100,\n",
    "                                shuffle=False)\n",
    "    test_dataloader = DataLoader(mnist_test,\n",
    "                                 batch_size=100,\n",
    "                                 shuffle=False)\n",
    "\n",
    "    '''\n",
    "    2. モデルの構築\n",
    "    '''\n",
    "    model = DNN(784, 200, 10).to(device)\n",
    "\n",
    "    '''\n",
    "    3. モデルの学習\n",
    "    '''\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizers.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    def compute_loss(t, y):\n",
    "        return criterion(y, t)\n",
    "\n",
    "    def train_step(x, t):\n",
    "        model.train()\n",
    "        preds = model(x)\n",
    "        loss = compute_loss(t, preds)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss, preds\n",
    "\n",
    "    def val_step(x, t):\n",
    "        model.eval()\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, t)\n",
    "\n",
    "        return loss, preds\n",
    "\n",
    "    epochs = 1000\n",
    "    hist = {'val_loss': [], 'val_accuracy': []}\n",
    "    es=EarlyStopping(patience=5,verbose=1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        val_loss = 0.\n",
    "        val_acc = 0.\n",
    "\n",
    "        for (x, t) in train_dataloader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            loss, preds = train_step(x, t)\n",
    "            train_loss += loss.item()\n",
    "            train_acc += \\\n",
    "                accuracy_score(t.tolist(),\n",
    "                               preds.argmax(dim=-1).tolist())\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc /= len(train_dataloader)\n",
    "\n",
    "        for (x, t) in val_dataloader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            loss, preds = val_step(x, t)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += \\\n",
    "                accuracy_score(t.tolist(),\n",
    "                               preds.argmax(dim=-1).tolist())\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_acc /= len(val_dataloader)\n",
    "\n",
    "        hist['val_loss'].append(val_loss)\n",
    "        hist['val_accuracy'].append(val_acc)\n",
    "\n",
    "        print('epoch: {}, loss: {:.3}, acc: {:.3f}'\n",
    "              ', val_loss: {:.3}, val_acc: {:.3f}'.format(\n",
    "                  epoch+1,\n",
    "                  train_loss,\n",
    "                  train_acc,\n",
    "                  val_loss,\n",
    "                  val_acc\n",
    "              ))\n",
    "        if es(val_loss):\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.087, test_acc: 0.976\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "    '''\n",
    "    4. モデルの評価\n",
    "    '''\n",
    "    # 検証データの誤差の可視化\n",
    "    val_loss = hist['val_loss']\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.rc('font', family='serif')\n",
    "    plt.plot(range(len(val_loss)), val_loss,\n",
    "             color='black', linewidth=1)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    # plt.savefig('output.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    # 正解率を可視化する場合\n",
    "    # val_acc = hist['val_accuracy']\n",
    "    #\n",
    "    # fig = plt.figure()\n",
    "    # plt.rc('font', family='serif')\n",
    "    # plt.plot(range(len(val_acc)), val_acc,\n",
    "    #          color='black', linewidth=1)\n",
    "    # plt.xlabel('epochs')\n",
    "    # plt.ylabel('acc')\n",
    "    # plt.savefig('output_acc.jpg')\n",
    "    # plt.show()\n",
    "\n",
    "    # テストデータの評価\n",
    "    def test_step(x, t):\n",
    "        return val_step(x, t)\n",
    "\n",
    "    test_loss = 0.\n",
    "    test_acc = 0.\n",
    "\n",
    "    for (x, t) in test_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, preds = test_step(x, t)\n",
    "        test_loss += loss.item()\n",
    "        test_acc += \\\n",
    "            accuracy_score(t.tolist(),\n",
    "                           preds.argmax(dim=-1).tolist())\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "    print('test_loss: {:.3f}, test_acc: {:.3f}'.format(\n",
    "        test_loss,\n",
    "        test_acc\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
