{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from callbacks import EarlyStopping\n",
    "            \n",
    "class DNN(Model):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = Dense(hidden_dim, activation='relu')\n",
    "        self.d1 = Dropout(0.5)\n",
    "        self.l2 = Dense(hidden_dim, activation='relu')\n",
    "        self.d2 = Dropout(0.5)\n",
    "        self.l3 = Dense(hidden_dim, activation='relu')\n",
    "        self.d3 = Dropout(0.5)\n",
    "        self.l4 = Dense(output_dim, activation='softmax')\n",
    "\n",
    "        self.ls = [self.l1, self.d1,\n",
    "                   self.l2, self.d2,\n",
    "                   self.l3, self.d3,\n",
    "                   self.l4]\n",
    "\n",
    "    def call(self, x):\n",
    "        for layer in self.ls:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(123)\n",
    "    tf.random.set_seed(123)\n",
    "\n",
    "    '''\n",
    "    1. データの準備\n",
    "    '''\n",
    "    mnist = datasets.mnist\n",
    "    (x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "    x_train = (x_train.reshape(-1, 784) / 255).astype(np.float32)\n",
    "    x_test = (x_test.reshape(-1, 784) / 255).astype(np.float32)\n",
    "\n",
    "    x_train, x_val, t_train, t_val = \\\n",
    "        train_test_split(x_train, t_train, test_size=0.2)\n",
    "\n",
    "    '''\n",
    "    2. モデルの構築\n",
    "    '''\n",
    "    model = DNN(200, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=losses.SparseCategoricalCrossentropy()\n",
    "optimizer=optimizers.SGD(learning_rate=0.01)\n",
    "train_loss=metrics.Mean()\n",
    "train_acc=metrics.SparseCategoricalAccuracy()\n",
    "val_loss=metrics.Mean()\n",
    "val_acc=metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "def compute_loss(t,y):\n",
    "    return criterion(t,y)\n",
    "\n",
    "def train_step(x,t):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds=model(x)\n",
    "        loss=compute_loss(t,preds)\n",
    "    grads=tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_acc=(t,preds)\n",
    "    return loss\n",
    "\n",
    "def val_step(x,t):\n",
    "    preds=model(x)\n",
    "    loss=compute_loss(t,preds)\n",
    "    val_loss(loss)\n",
    "    val_acc(t,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.22, acc: 0.000, val_loss: 0.548, val_acc: 0.857\n",
      "epoch: 2, loss: 0.829, acc: 0.000, val_loss: 0.465, val_acc: 0.873\n",
      "epoch: 3, loss: 0.669, acc: 0.000, val_loss: 0.42, val_acc: 0.883\n",
      "epoch: 4, loss: 0.579, acc: 0.000, val_loss: 0.39, val_acc: 0.890\n",
      "epoch: 5, loss: 0.52, acc: 0.000, val_loss: 0.368, val_acc: 0.896\n",
      "epoch: 6, loss: 0.477, acc: 0.000, val_loss: 0.35, val_acc: 0.900\n",
      "epoch: 7, loss: 0.443, acc: 0.000, val_loss: 0.335, val_acc: 0.904\n",
      "epoch: 8, loss: 0.417, acc: 0.000, val_loss: 0.323, val_acc: 0.907\n",
      "epoch: 9, loss: 0.394, acc: 0.000, val_loss: 0.312, val_acc: 0.910\n",
      "epoch: 10, loss: 0.375, acc: 0.000, val_loss: 0.302, val_acc: 0.913\n",
      "epoch: 11, loss: 0.359, acc: 0.000, val_loss: 0.293, val_acc: 0.915\n",
      "epoch: 12, loss: 0.344, acc: 0.000, val_loss: 0.285, val_acc: 0.918\n",
      "epoch: 13, loss: 0.331, acc: 0.000, val_loss: 0.277, val_acc: 0.920\n",
      "epoch: 14, loss: 0.32, acc: 0.000, val_loss: 0.27, val_acc: 0.921\n",
      "epoch: 15, loss: 0.309, acc: 0.000, val_loss: 0.264, val_acc: 0.923\n",
      "epoch: 16, loss: 0.299, acc: 0.000, val_loss: 0.258, val_acc: 0.925\n",
      "epoch: 17, loss: 0.29, acc: 0.000, val_loss: 0.252, val_acc: 0.926\n",
      "epoch: 18, loss: 0.282, acc: 0.000, val_loss: 0.247, val_acc: 0.928\n",
      "epoch: 19, loss: 0.274, acc: 0.000, val_loss: 0.242, val_acc: 0.929\n",
      "epoch: 20, loss: 0.267, acc: 0.000, val_loss: 0.237, val_acc: 0.930\n",
      "epoch: 21, loss: 0.26, acc: 0.000, val_loss: 0.232, val_acc: 0.932\n",
      "epoch: 22, loss: 0.254, acc: 0.000, val_loss: 0.228, val_acc: 0.933\n",
      "epoch: 23, loss: 0.248, acc: 0.000, val_loss: 0.224, val_acc: 0.934\n",
      "epoch: 24, loss: 0.242, acc: 0.000, val_loss: 0.22, val_acc: 0.935\n",
      "epoch: 25, loss: 0.236, acc: 0.000, val_loss: 0.217, val_acc: 0.936\n",
      "epoch: 26, loss: 0.231, acc: 0.000, val_loss: 0.213, val_acc: 0.937\n",
      "epoch: 27, loss: 0.226, acc: 0.000, val_loss: 0.21, val_acc: 0.938\n",
      "epoch: 28, loss: 0.222, acc: 0.000, val_loss: 0.207, val_acc: 0.939\n",
      "epoch: 29, loss: 0.217, acc: 0.000, val_loss: 0.204, val_acc: 0.940\n",
      "epoch: 30, loss: 0.213, acc: 0.000, val_loss: 0.201, val_acc: 0.941\n",
      "epoch: 31, loss: 0.209, acc: 0.000, val_loss: 0.198, val_acc: 0.942\n",
      "epoch: 32, loss: 0.205, acc: 0.000, val_loss: 0.195, val_acc: 0.942\n",
      "epoch: 33, loss: 0.201, acc: 0.000, val_loss: 0.193, val_acc: 0.943\n",
      "epoch: 34, loss: 0.198, acc: 0.000, val_loss: 0.19, val_acc: 0.944\n",
      "epoch: 35, loss: 0.194, acc: 0.000, val_loss: 0.188, val_acc: 0.944\n",
      "epoch: 36, loss: 0.191, acc: 0.000, val_loss: 0.185, val_acc: 0.945\n",
      "epoch: 37, loss: 0.187, acc: 0.000, val_loss: 0.183, val_acc: 0.946\n",
      "epoch: 38, loss: 0.184, acc: 0.000, val_loss: 0.181, val_acc: 0.946\n",
      "epoch: 39, loss: 0.181, acc: 0.000, val_loss: 0.179, val_acc: 0.947\n",
      "epoch: 40, loss: 0.178, acc: 0.000, val_loss: 0.177, val_acc: 0.947\n",
      "epoch: 41, loss: 0.175, acc: 0.000, val_loss: 0.175, val_acc: 0.948\n",
      "epoch: 42, loss: 0.173, acc: 0.000, val_loss: 0.173, val_acc: 0.949\n",
      "epoch: 43, loss: 0.17, acc: 0.000, val_loss: 0.172, val_acc: 0.949\n",
      "epoch: 44, loss: 0.167, acc: 0.000, val_loss: 0.17, val_acc: 0.950\n",
      "epoch: 45, loss: 0.165, acc: 0.000, val_loss: 0.168, val_acc: 0.950\n",
      "epoch: 46, loss: 0.162, acc: 0.000, val_loss: 0.167, val_acc: 0.950\n",
      "epoch: 47, loss: 0.16, acc: 0.000, val_loss: 0.165, val_acc: 0.951\n",
      "epoch: 48, loss: 0.158, acc: 0.000, val_loss: 0.164, val_acc: 0.951\n",
      "epoch: 49, loss: 0.155, acc: 0.000, val_loss: 0.162, val_acc: 0.952\n",
      "epoch: 50, loss: 0.153, acc: 0.000, val_loss: 0.161, val_acc: 0.952\n",
      "epoch: 51, loss: 0.151, acc: 0.000, val_loss: 0.16, val_acc: 0.952\n",
      "epoch: 52, loss: 0.149, acc: 0.000, val_loss: 0.158, val_acc: 0.953\n",
      "epoch: 53, loss: 0.147, acc: 0.000, val_loss: 0.157, val_acc: 0.953\n",
      "epoch: 54, loss: 0.145, acc: 0.000, val_loss: 0.156, val_acc: 0.953\n",
      "epoch: 55, loss: 0.143, acc: 0.000, val_loss: 0.155, val_acc: 0.954\n",
      "epoch: 56, loss: 0.141, acc: 0.000, val_loss: 0.153, val_acc: 0.954\n",
      "epoch: 57, loss: 0.139, acc: 0.000, val_loss: 0.152, val_acc: 0.954\n",
      "epoch: 58, loss: 0.138, acc: 0.000, val_loss: 0.151, val_acc: 0.955\n",
      "epoch: 59, loss: 0.136, acc: 0.000, val_loss: 0.15, val_acc: 0.955\n",
      "epoch: 60, loss: 0.134, acc: 0.000, val_loss: 0.149, val_acc: 0.955\n",
      "epoch: 61, loss: 0.133, acc: 0.000, val_loss: 0.148, val_acc: 0.956\n",
      "epoch: 62, loss: 0.131, acc: 0.000, val_loss: 0.147, val_acc: 0.956\n",
      "epoch: 63, loss: 0.129, acc: 0.000, val_loss: 0.146, val_acc: 0.956\n",
      "epoch: 64, loss: 0.128, acc: 0.000, val_loss: 0.145, val_acc: 0.956\n",
      "epoch: 65, loss: 0.126, acc: 0.000, val_loss: 0.144, val_acc: 0.957\n",
      "epoch: 66, loss: 0.125, acc: 0.000, val_loss: 0.143, val_acc: 0.957\n",
      "epoch: 67, loss: 0.123, acc: 0.000, val_loss: 0.143, val_acc: 0.957\n",
      "epoch: 68, loss: 0.122, acc: 0.000, val_loss: 0.142, val_acc: 0.957\n",
      "epoch: 69, loss: 0.121, acc: 0.000, val_loss: 0.141, val_acc: 0.957\n",
      "epoch: 70, loss: 0.119, acc: 0.000, val_loss: 0.14, val_acc: 0.958\n",
      "epoch: 71, loss: 0.118, acc: 0.000, val_loss: 0.139, val_acc: 0.958\n",
      "epoch: 72, loss: 0.116, acc: 0.000, val_loss: 0.139, val_acc: 0.958\n",
      "epoch: 73, loss: 0.115, acc: 0.000, val_loss: 0.138, val_acc: 0.958\n",
      "epoch: 74, loss: 0.114, acc: 0.000, val_loss: 0.137, val_acc: 0.959\n",
      "epoch: 75, loss: 0.113, acc: 0.000, val_loss: 0.137, val_acc: 0.959\n",
      "epoch: 76, loss: 0.111, acc: 0.000, val_loss: 0.136, val_acc: 0.959\n",
      "epoch: 77, loss: 0.11, acc: 0.000, val_loss: 0.135, val_acc: 0.959\n",
      "epoch: 78, loss: 0.109, acc: 0.000, val_loss: 0.135, val_acc: 0.959\n",
      "epoch: 79, loss: 0.108, acc: 0.000, val_loss: 0.134, val_acc: 0.959\n",
      "epoch: 80, loss: 0.107, acc: 0.000, val_loss: 0.134, val_acc: 0.960\n",
      "epoch: 81, loss: 0.106, acc: 0.000, val_loss: 0.133, val_acc: 0.960\n",
      "epoch: 82, loss: 0.105, acc: 0.000, val_loss: 0.132, val_acc: 0.960\n",
      "epoch: 83, loss: 0.104, acc: 0.000, val_loss: 0.132, val_acc: 0.960\n",
      "epoch: 84, loss: 0.103, acc: 0.000, val_loss: 0.131, val_acc: 0.960\n",
      "epoch: 85, loss: 0.102, acc: 0.000, val_loss: 0.131, val_acc: 0.960\n",
      "epoch: 86, loss: 0.101, acc: 0.000, val_loss: 0.13, val_acc: 0.961\n",
      "epoch: 87, loss: 0.0996, acc: 0.000, val_loss: 0.13, val_acc: 0.961\n",
      "epoch: 88, loss: 0.0986, acc: 0.000, val_loss: 0.129, val_acc: 0.961\n",
      "epoch: 89, loss: 0.0976, acc: 0.000, val_loss: 0.129, val_acc: 0.961\n",
      "epoch: 90, loss: 0.0967, acc: 0.000, val_loss: 0.128, val_acc: 0.961\n",
      "epoch: 91, loss: 0.0958, acc: 0.000, val_loss: 0.128, val_acc: 0.961\n",
      "epoch: 92, loss: 0.0949, acc: 0.000, val_loss: 0.128, val_acc: 0.961\n",
      "epoch: 93, loss: 0.094, acc: 0.000, val_loss: 0.127, val_acc: 0.962\n",
      "epoch: 94, loss: 0.0931, acc: 0.000, val_loss: 0.127, val_acc: 0.962\n",
      "epoch: 95, loss: 0.0923, acc: 0.000, val_loss: 0.126, val_acc: 0.962\n",
      "epoch: 96, loss: 0.0914, acc: 0.000, val_loss: 0.126, val_acc: 0.962\n",
      "epoch: 97, loss: 0.0906, acc: 0.000, val_loss: 0.126, val_acc: 0.962\n",
      "epoch: 98, loss: 0.0898, acc: 0.000, val_loss: 0.125, val_acc: 0.962\n",
      "epoch: 99, loss: 0.089, acc: 0.000, val_loss: 0.125, val_acc: 0.962\n",
      "epoch: 100, loss: 0.0882, acc: 0.000, val_loss: 0.125, val_acc: 0.962\n",
      "epoch: 101, loss: 0.0874, acc: 0.000, val_loss: 0.124, val_acc: 0.962\n",
      "epoch: 102, loss: 0.0866, acc: 0.000, val_loss: 0.124, val_acc: 0.963\n",
      "epoch: 103, loss: 0.0859, acc: 0.000, val_loss: 0.124, val_acc: 0.963\n",
      "epoch: 104, loss: 0.0851, acc: 0.000, val_loss: 0.123, val_acc: 0.963\n",
      "epoch: 105, loss: 0.0844, acc: 0.000, val_loss: 0.123, val_acc: 0.963\n",
      "epoch: 106, loss: 0.0837, acc: 0.000, val_loss: 0.123, val_acc: 0.963\n",
      "epoch: 107, loss: 0.083, acc: 0.000, val_loss: 0.122, val_acc: 0.963\n",
      "epoch: 108, loss: 0.0823, acc: 0.000, val_loss: 0.122, val_acc: 0.963\n",
      "epoch: 109, loss: 0.0816, acc: 0.000, val_loss: 0.122, val_acc: 0.963\n",
      "epoch: 110, loss: 0.081, acc: 0.000, val_loss: 0.121, val_acc: 0.963\n",
      "epoch: 111, loss: 0.0803, acc: 0.000, val_loss: 0.121, val_acc: 0.964\n",
      "epoch: 112, loss: 0.0796, acc: 0.000, val_loss: 0.121, val_acc: 0.964\n",
      "epoch: 113, loss: 0.079, acc: 0.000, val_loss: 0.121, val_acc: 0.964\n",
      "epoch: 114, loss: 0.0784, acc: 0.000, val_loss: 0.12, val_acc: 0.964\n",
      "epoch: 115, loss: 0.0777, acc: 0.000, val_loss: 0.12, val_acc: 0.964\n",
      "epoch: 116, loss: 0.0771, acc: 0.000, val_loss: 0.12, val_acc: 0.964\n",
      "epoch: 117, loss: 0.0765, acc: 0.000, val_loss: 0.12, val_acc: 0.964\n",
      "epoch: 118, loss: 0.0759, acc: 0.000, val_loss: 0.119, val_acc: 0.964\n",
      "epoch: 119, loss: 0.0753, acc: 0.000, val_loss: 0.119, val_acc: 0.964\n",
      "epoch: 120, loss: 0.0748, acc: 0.000, val_loss: 0.119, val_acc: 0.964\n",
      "epoch: 121, loss: 0.0742, acc: 0.000, val_loss: 0.119, val_acc: 0.964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 122, loss: 0.0736, acc: 0.000, val_loss: 0.119, val_acc: 0.964\n",
      "epoch: 123, loss: 0.0731, acc: 0.000, val_loss: 0.118, val_acc: 0.965\n",
      "epoch: 124, loss: 0.0725, acc: 0.000, val_loss: 0.118, val_acc: 0.965\n",
      "epoch: 125, loss: 0.072, acc: 0.000, val_loss: 0.118, val_acc: 0.965\n",
      "epoch: 126, loss: 0.0715, acc: 0.000, val_loss: 0.118, val_acc: 0.965\n",
      "epoch: 127, loss: 0.0709, acc: 0.000, val_loss: 0.118, val_acc: 0.965\n",
      "epoch: 128, loss: 0.0704, acc: 0.000, val_loss: 0.117, val_acc: 0.965\n",
      "epoch: 129, loss: 0.0699, acc: 0.000, val_loss: 0.117, val_acc: 0.965\n",
      "epoch: 130, loss: 0.0694, acc: 0.000, val_loss: 0.117, val_acc: 0.965\n",
      "epoch: 131, loss: 0.0689, acc: 0.000, val_loss: 0.117, val_acc: 0.965\n",
      "epoch: 132, loss: 0.0684, acc: 0.000, val_loss: 0.117, val_acc: 0.965\n",
      "epoch: 133, loss: 0.068, acc: 0.000, val_loss: 0.117, val_acc: 0.965\n",
      "epoch: 134, loss: 0.0675, acc: 0.000, val_loss: 0.116, val_acc: 0.965\n",
      "epoch: 135, loss: 0.067, acc: 0.000, val_loss: 0.116, val_acc: 0.965\n",
      "epoch: 136, loss: 0.0666, acc: 0.000, val_loss: 0.116, val_acc: 0.965\n",
      "epoch: 137, loss: 0.0661, acc: 0.000, val_loss: 0.116, val_acc: 0.966\n",
      "epoch: 138, loss: 0.0656, acc: 0.000, val_loss: 0.116, val_acc: 0.966\n",
      "epoch: 139, loss: 0.0652, acc: 0.000, val_loss: 0.116, val_acc: 0.966\n",
      "epoch: 140, loss: 0.0648, acc: 0.000, val_loss: 0.115, val_acc: 0.966\n",
      "epoch: 141, loss: 0.0643, acc: 0.000, val_loss: 0.115, val_acc: 0.966\n",
      "epoch: 142, loss: 0.0639, acc: 0.000, val_loss: 0.115, val_acc: 0.966\n",
      "epoch: 143, loss: 0.0635, acc: 0.000, val_loss: 0.115, val_acc: 0.966\n",
      "epoch: 144, loss: 0.0631, acc: 0.000, val_loss: 0.115, val_acc: 0.966\n",
      "epoch: 145, loss: 0.0627, acc: 0.000, val_loss: 0.115, val_acc: 0.966\n",
      "epoch: 146, loss: 0.0623, acc: 0.000, val_loss: 0.115, val_acc: 0.966\n",
      "epoch: 147, loss: 0.0619, acc: 0.000, val_loss: 0.115, val_acc: 0.966\n",
      "epoch: 148, loss: 0.0615, acc: 0.000, val_loss: 0.114, val_acc: 0.966\n",
      "epoch: 149, loss: 0.0611, acc: 0.000, val_loss: 0.114, val_acc: 0.966\n",
      "epoch: 150, loss: 0.0607, acc: 0.000, val_loss: 0.114, val_acc: 0.966\n",
      "epoch: 151, loss: 0.0603, acc: 0.000, val_loss: 0.114, val_acc: 0.966\n",
      "epoch: 152, loss: 0.0599, acc: 0.000, val_loss: 0.114, val_acc: 0.966\n",
      "epoch: 153, loss: 0.0596, acc: 0.000, val_loss: 0.114, val_acc: 0.966\n",
      "epoch: 154, loss: 0.0592, acc: 0.000, val_loss: 0.114, val_acc: 0.966\n",
      "epoch: 155, loss: 0.0588, acc: 0.000, val_loss: 0.114, val_acc: 0.966\n",
      "epoch: 156, loss: 0.0585, acc: 0.000, val_loss: 0.114, val_acc: 0.967\n",
      "epoch: 157, loss: 0.0581, acc: 0.000, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 158, loss: 0.0578, acc: 0.000, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 159, loss: 0.0574, acc: 0.000, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 160, loss: 0.0571, acc: 0.000, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 161, loss: 0.0567, acc: 0.000, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 162, loss: 0.0564, acc: 0.000, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 163, loss: 0.0561, acc: 0.000, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 164, loss: 0.0557, acc: 0.000, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 165, loss: 0.0554, acc: 0.000, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 166, loss: 0.0551, acc: 0.000, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 167, loss: 0.0548, acc: 0.000, val_loss: 0.113, val_acc: 0.967\n",
      "epoch: 168, loss: 0.0545, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 169, loss: 0.0542, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 170, loss: 0.0539, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 171, loss: 0.0536, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 172, loss: 0.0533, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 173, loss: 0.053, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 174, loss: 0.0527, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 175, loss: 0.0524, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 176, loss: 0.0521, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 177, loss: 0.0518, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 178, loss: 0.0515, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 179, loss: 0.0513, acc: 0.000, val_loss: 0.112, val_acc: 0.967\n",
      "epoch: 180, loss: 0.051, acc: 0.000, val_loss: 0.112, val_acc: 0.968\n",
      "epoch: 181, loss: 0.0507, acc: 0.000, val_loss: 0.112, val_acc: 0.968\n",
      "epoch: 182, loss: 0.0505, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 183, loss: 0.0502, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 184, loss: 0.0499, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 185, loss: 0.0497, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 186, loss: 0.0494, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 187, loss: 0.0492, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 188, loss: 0.0489, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 189, loss: 0.0487, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 190, loss: 0.0484, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 191, loss: 0.0482, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 192, loss: 0.0479, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 193, loss: 0.0477, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 194, loss: 0.0474, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 195, loss: 0.0472, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 196, loss: 0.047, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 197, loss: 0.0468, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 198, loss: 0.0465, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 199, loss: 0.0463, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 200, loss: 0.0461, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 201, loss: 0.0459, acc: 0.000, val_loss: 0.111, val_acc: 0.968\n",
      "epoch: 202, loss: 0.0456, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 203, loss: 0.0454, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 204, loss: 0.0452, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 205, loss: 0.045, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 206, loss: 0.0448, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 207, loss: 0.0446, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 208, loss: 0.0444, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 209, loss: 0.0442, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 210, loss: 0.044, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 211, loss: 0.0438, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 212, loss: 0.0436, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 213, loss: 0.0434, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 214, loss: 0.0432, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 215, loss: 0.043, acc: 0.000, val_loss: 0.11, val_acc: 0.968\n",
      "epoch: 216, loss: 0.0428, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 217, loss: 0.0426, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 218, loss: 0.0424, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 219, loss: 0.0422, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 220, loss: 0.042, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 221, loss: 0.0418, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 222, loss: 0.0417, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 223, loss: 0.0415, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 224, loss: 0.0413, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 225, loss: 0.0411, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 226, loss: 0.0409, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 227, loss: 0.0408, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 228, loss: 0.0406, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 229, loss: 0.0404, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 230, loss: 0.0402, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 231, loss: 0.0401, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 232, loss: 0.0399, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 233, loss: 0.0397, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 234, loss: 0.0396, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 235, loss: 0.0394, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 236, loss: 0.0393, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 237, loss: 0.0391, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 238, loss: 0.0389, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 239, loss: 0.0388, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 240, loss: 0.0386, acc: 0.000, val_loss: 0.11, val_acc: 0.969\n",
      "epoch: 241, loss: 0.0385, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 242, loss: 0.0383, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 243, loss: 0.0382, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 244, loss: 0.038, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 245, loss: 0.0379, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 246, loss: 0.0377, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 247, loss: 0.0376, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 248, loss: 0.0374, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 249, loss: 0.0373, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 250, loss: 0.0371, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 251, loss: 0.037, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 252, loss: 0.0368, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 253, loss: 0.0367, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 254, loss: 0.0365, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 255, loss: 0.0364, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 256, loss: 0.0363, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 257, loss: 0.0361, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 258, loss: 0.036, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 259, loss: 0.0359, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 260, loss: 0.0357, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 261, loss: 0.0356, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 262, loss: 0.0355, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 263, loss: 0.0353, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 264, loss: 0.0352, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 265, loss: 0.0351, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 266, loss: 0.0349, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 267, loss: 0.0348, acc: 0.000, val_loss: 0.109, val_acc: 0.969\n",
      "epoch: 268, loss: 0.0347, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 269, loss: 0.0346, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 270, loss: 0.0344, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 271, loss: 0.0343, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 272, loss: 0.0342, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 273, loss: 0.0341, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 274, loss: 0.0339, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 275, loss: 0.0338, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 276, loss: 0.0337, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 277, loss: 0.0336, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 278, loss: 0.0335, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 279, loss: 0.0334, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 280, loss: 0.0332, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 281, loss: 0.0331, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 282, loss: 0.033, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 283, loss: 0.0329, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 284, loss: 0.0328, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 285, loss: 0.0327, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 286, loss: 0.0326, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 287, loss: 0.0324, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n",
      "epoch: 288, loss: 0.0323, acc: 0.000, val_loss: 0.109, val_acc: 0.970\n"
     ]
    }
   ],
   "source": [
    "x_,t_=shuffle(x_train,t_train)\n",
    "es=EarlyStopping()\n",
    "epochs=1000\n",
    "batch_size=100\n",
    "n_batches_train=x_train.shape[0]//batch_size\n",
    "n_batches_val=x_val.shape[0]//batch_size\n",
    "hist={'val_loss':[],'val_accuracy':[]}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for batch in range(n_batches_train):\n",
    "        start=batch*batch_size\n",
    "        end=start+batch_size\n",
    "        train_step(x_[start:end],t_[start:end])\n",
    "    \n",
    "    for batch in range(n_batches_val):\n",
    "        start=batch*batch_size\n",
    "        end=start+batch_size\n",
    "        val_step(x_val[start:end],t_val[start:end])\n",
    "        \n",
    "    hist['val_loss'].append(val_loss.result())\n",
    "    hist['val_accuracy'].append(val_acc.result())\n",
    "  \n",
    "    print('epoch: {}, loss: {:.3}, acc: {:.3f}, val_loss: {:.3}, val_acc: {:.3f}'.format(epoch+1,train_loss.result(),train_acc.result(),val_loss.result(),val_acc.result()))\n",
    "\n",
    "    if es(val_loss.result()):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loss=hist['val_loss']\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.rc('font',family='serif')\n",
    "plt.plot(range(len(val_loss)),val_loss,color='black',linewidth=1)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.112, test_acc: 0.975\n"
     ]
    }
   ],
   "source": [
    "test_loss=metrics.Mean()\n",
    "test_acc=metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "def test_step(x,t):\n",
    "    preds=model(x)\n",
    "    loss=compute_loss(t,preds)\n",
    "    test_loss(loss)\n",
    "    test_acc(t,preds)\n",
    "    return loss\n",
    "\n",
    "test_step(x_test,t_test)\n",
    "\n",
    "print('test_loss: {:.3}, test_acc: {:.3f}'.format(test_loss.result(),test_acc.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
