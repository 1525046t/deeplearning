{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.l1=nn.Linear(input_dim,hidden_dim)\n",
    "        self.a1=nn.Sigmoid()\n",
    "        self.l2=nn.Linear(hidden_dim,output_dim)\n",
    "        self.a2=nn.Sigmoid()\n",
    "        \n",
    "        self.layers=[self.l1,self.a1,self.l2,self.a2]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=300\n",
    "x,t=datasets.make_moons(N,noise=0.3)\n",
    "t=t.reshape(N,1)\n",
    "x_train,x_test,t_train,t_test=train_test_split(x,t,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.45\n",
      "epoch: 2, loss: 1.44\n",
      "epoch: 3, loss: 1.42\n",
      "epoch: 4, loss: 1.43\n",
      "epoch: 5, loss: 1.42\n",
      "epoch: 6, loss: 1.42\n",
      "epoch: 7, loss: 1.42\n",
      "epoch: 8, loss: 1.42\n",
      "epoch: 9, loss: 1.42\n",
      "epoch: 10, loss: 1.41\n",
      "epoch: 11, loss: 1.4\n",
      "epoch: 12, loss: 1.41\n",
      "epoch: 13, loss: 1.42\n",
      "epoch: 14, loss: 1.41\n",
      "epoch: 15, loss: 1.4\n",
      "epoch: 16, loss: 1.41\n",
      "epoch: 17, loss: 1.4\n",
      "epoch: 18, loss: 1.4\n",
      "epoch: 19, loss: 1.4\n",
      "epoch: 20, loss: 1.39\n",
      "epoch: 21, loss: 1.38\n",
      "epoch: 22, loss: 1.4\n",
      "epoch: 23, loss: 1.39\n",
      "epoch: 24, loss: 1.39\n",
      "epoch: 25, loss: 1.39\n",
      "epoch: 26, loss: 1.39\n",
      "epoch: 27, loss: 1.38\n",
      "epoch: 28, loss: 1.39\n",
      "epoch: 29, loss: 1.38\n",
      "epoch: 30, loss: 1.37\n",
      "epoch: 31, loss: 1.38\n",
      "epoch: 32, loss: 1.37\n",
      "epoch: 33, loss: 1.37\n",
      "epoch: 34, loss: 1.38\n",
      "epoch: 35, loss: 1.37\n",
      "epoch: 36, loss: 1.37\n",
      "epoch: 37, loss: 1.37\n",
      "epoch: 38, loss: 1.36\n",
      "epoch: 39, loss: 1.37\n",
      "epoch: 40, loss: 1.37\n",
      "epoch: 41, loss: 1.36\n",
      "epoch: 42, loss: 1.37\n",
      "epoch: 43, loss: 1.36\n",
      "epoch: 44, loss: 1.36\n",
      "epoch: 45, loss: 1.35\n",
      "epoch: 46, loss: 1.35\n",
      "epoch: 47, loss: 1.35\n",
      "epoch: 48, loss: 1.36\n",
      "epoch: 49, loss: 1.35\n",
      "epoch: 50, loss: 1.35\n",
      "epoch: 51, loss: 1.35\n",
      "epoch: 52, loss: 1.34\n",
      "epoch: 53, loss: 1.34\n",
      "epoch: 54, loss: 1.34\n",
      "epoch: 55, loss: 1.34\n",
      "epoch: 56, loss: 1.34\n",
      "epoch: 57, loss: 1.34\n",
      "epoch: 58, loss: 1.33\n",
      "epoch: 59, loss: 1.34\n",
      "epoch: 60, loss: 1.33\n",
      "epoch: 61, loss: 1.33\n",
      "epoch: 62, loss: 1.33\n",
      "epoch: 63, loss: 1.33\n",
      "epoch: 64, loss: 1.33\n",
      "epoch: 65, loss: 1.32\n",
      "epoch: 66, loss: 1.33\n",
      "epoch: 67, loss: 1.31\n",
      "epoch: 68, loss: 1.33\n",
      "epoch: 69, loss: 1.32\n",
      "epoch: 70, loss: 1.31\n",
      "epoch: 71, loss: 1.31\n",
      "epoch: 72, loss: 1.31\n",
      "epoch: 73, loss: 1.31\n",
      "epoch: 74, loss: 1.31\n",
      "epoch: 75, loss: 1.3\n",
      "epoch: 76, loss: 1.3\n",
      "epoch: 77, loss: 1.29\n",
      "epoch: 78, loss: 1.3\n",
      "epoch: 79, loss: 1.29\n",
      "epoch: 80, loss: 1.29\n",
      "epoch: 81, loss: 1.29\n",
      "epoch: 82, loss: 1.29\n",
      "epoch: 83, loss: 1.28\n",
      "epoch: 84, loss: 1.29\n",
      "epoch: 85, loss: 1.28\n",
      "epoch: 86, loss: 1.28\n",
      "epoch: 87, loss: 1.28\n",
      "epoch: 88, loss: 1.27\n",
      "epoch: 89, loss: 1.28\n",
      "epoch: 90, loss: 1.27\n",
      "epoch: 91, loss: 1.27\n",
      "epoch: 92, loss: 1.27\n",
      "epoch: 93, loss: 1.26\n",
      "epoch: 94, loss: 1.27\n",
      "epoch: 95, loss: 1.26\n",
      "epoch: 96, loss: 1.26\n",
      "epoch: 97, loss: 1.24\n",
      "epoch: 98, loss: 1.25\n",
      "epoch: 99, loss: 1.24\n",
      "epoch: 100, loss: 1.24\n",
      "epoch: 101, loss: 1.26\n",
      "epoch: 102, loss: 1.25\n",
      "epoch: 103, loss: 1.24\n",
      "epoch: 104, loss: 1.24\n",
      "epoch: 105, loss: 1.25\n",
      "epoch: 106, loss: 1.23\n",
      "epoch: 107, loss: 1.23\n",
      "epoch: 108, loss: 1.23\n",
      "epoch: 109, loss: 1.23\n",
      "epoch: 110, loss: 1.23\n",
      "epoch: 111, loss: 1.21\n",
      "epoch: 112, loss: 1.22\n",
      "epoch: 113, loss: 1.22\n",
      "epoch: 114, loss: 1.21\n",
      "epoch: 115, loss: 1.2\n",
      "epoch: 116, loss: 1.21\n",
      "epoch: 117, loss: 1.21\n",
      "epoch: 118, loss: 1.21\n",
      "epoch: 119, loss: 1.2\n",
      "epoch: 120, loss: 1.19\n",
      "epoch: 121, loss: 1.2\n",
      "epoch: 122, loss: 1.18\n",
      "epoch: 123, loss: 1.19\n",
      "epoch: 124, loss: 1.19\n",
      "epoch: 125, loss: 1.19\n",
      "epoch: 126, loss: 1.18\n",
      "epoch: 127, loss: 1.18\n",
      "epoch: 128, loss: 1.17\n",
      "epoch: 129, loss: 1.17\n",
      "epoch: 130, loss: 1.17\n",
      "epoch: 131, loss: 1.17\n",
      "epoch: 132, loss: 1.15\n",
      "epoch: 133, loss: 1.16\n",
      "epoch: 134, loss: 1.15\n",
      "epoch: 135, loss: 1.13\n",
      "epoch: 136, loss: 1.15\n",
      "epoch: 137, loss: 1.14\n",
      "epoch: 138, loss: 1.14\n",
      "epoch: 139, loss: 1.15\n",
      "epoch: 140, loss: 1.13\n",
      "epoch: 141, loss: 1.13\n",
      "epoch: 142, loss: 1.12\n",
      "epoch: 143, loss: 1.13\n",
      "epoch: 144, loss: 1.11\n",
      "epoch: 145, loss: 1.12\n",
      "epoch: 146, loss: 1.13\n",
      "epoch: 147, loss: 1.1\n",
      "epoch: 148, loss: 1.12\n",
      "epoch: 149, loss: 1.12\n",
      "epoch: 150, loss: 1.09\n",
      "epoch: 151, loss: 1.12\n",
      "epoch: 152, loss: 1.09\n",
      "epoch: 153, loss: 1.08\n",
      "epoch: 154, loss: 1.11\n",
      "epoch: 155, loss: 1.09\n",
      "epoch: 156, loss: 1.1\n",
      "epoch: 157, loss: 1.09\n",
      "epoch: 158, loss: 1.09\n",
      "epoch: 159, loss: 1.09\n",
      "epoch: 160, loss: 1.08\n",
      "epoch: 161, loss: 1.06\n",
      "epoch: 162, loss: 1.06\n",
      "epoch: 163, loss: 1.08\n",
      "epoch: 164, loss: 1.07\n",
      "epoch: 165, loss: 1.08\n",
      "epoch: 166, loss: 1.05\n",
      "epoch: 167, loss: 1.06\n",
      "epoch: 168, loss: 1.06\n",
      "epoch: 169, loss: 1.03\n",
      "epoch: 170, loss: 1.04\n",
      "epoch: 171, loss: 1.04\n",
      "epoch: 172, loss: 1.05\n",
      "epoch: 173, loss: 1.07\n",
      "epoch: 174, loss: 1.02\n",
      "epoch: 175, loss: 1.03\n",
      "epoch: 176, loss: 1.05\n",
      "epoch: 177, loss: 1.02\n",
      "epoch: 178, loss: 1.02\n",
      "epoch: 179, loss: 1.04\n",
      "epoch: 180, loss: 1.04\n",
      "epoch: 181, loss: 1.02\n",
      "epoch: 182, loss: 1.01\n",
      "epoch: 183, loss: 1.04\n",
      "epoch: 184, loss: 1.0\n",
      "epoch: 185, loss: 1.01\n",
      "epoch: 186, loss: 1.0\n",
      "epoch: 187, loss: 1.01\n",
      "epoch: 188, loss: 1.0\n",
      "epoch: 189, loss: 0.995\n",
      "epoch: 190, loss: 0.993\n",
      "epoch: 191, loss: 0.999\n",
      "epoch: 192, loss: 1.0\n",
      "epoch: 193, loss: 1.01\n",
      "epoch: 194, loss: 0.985\n",
      "epoch: 195, loss: 0.998\n",
      "epoch: 196, loss: 1.02\n",
      "epoch: 197, loss: 0.972\n",
      "epoch: 198, loss: 0.991\n",
      "epoch: 199, loss: 0.989\n",
      "epoch: 200, loss: 0.967\n",
      "epoch: 201, loss: 0.981\n",
      "epoch: 202, loss: 0.975\n",
      "epoch: 203, loss: 0.964\n",
      "epoch: 204, loss: 0.973\n",
      "epoch: 205, loss: 0.961\n",
      "epoch: 206, loss: 0.994\n",
      "epoch: 207, loss: 0.979\n",
      "epoch: 208, loss: 0.958\n",
      "epoch: 209, loss: 0.962\n",
      "epoch: 210, loss: 0.965\n",
      "epoch: 211, loss: 0.933\n",
      "epoch: 212, loss: 0.943\n",
      "epoch: 213, loss: 0.962\n",
      "epoch: 214, loss: 0.945\n",
      "epoch: 215, loss: 0.933\n",
      "epoch: 216, loss: 0.957\n",
      "epoch: 217, loss: 0.947\n",
      "epoch: 218, loss: 0.938\n",
      "epoch: 219, loss: 0.945\n",
      "epoch: 220, loss: 0.924\n",
      "epoch: 221, loss: 0.925\n",
      "epoch: 222, loss: 0.957\n",
      "epoch: 223, loss: 0.929\n",
      "epoch: 224, loss: 0.932\n",
      "epoch: 225, loss: 0.941\n",
      "epoch: 226, loss: 0.949\n",
      "epoch: 227, loss: 0.957\n",
      "epoch: 228, loss: 0.909\n",
      "epoch: 229, loss: 0.946\n",
      "epoch: 230, loss: 0.923\n",
      "epoch: 231, loss: 0.936\n",
      "epoch: 232, loss: 0.944\n",
      "epoch: 233, loss: 0.894\n",
      "epoch: 234, loss: 0.954\n",
      "epoch: 235, loss: 0.909\n",
      "epoch: 236, loss: 0.894\n",
      "epoch: 237, loss: 0.926\n",
      "epoch: 238, loss: 0.949\n",
      "epoch: 239, loss: 0.952\n",
      "epoch: 240, loss: 0.893\n",
      "epoch: 241, loss: 0.881\n",
      "epoch: 242, loss: 0.917\n",
      "epoch: 243, loss: 0.964\n",
      "epoch: 244, loss: 0.908\n",
      "epoch: 245, loss: 0.926\n",
      "epoch: 246, loss: 0.907\n",
      "epoch: 247, loss: 0.922\n",
      "epoch: 248, loss: 0.897\n",
      "epoch: 249, loss: 0.893\n",
      "epoch: 250, loss: 0.901\n",
      "epoch: 251, loss: 0.921\n",
      "epoch: 252, loss: 0.938\n",
      "epoch: 253, loss: 0.937\n",
      "epoch: 254, loss: 0.893\n",
      "epoch: 255, loss: 0.9\n",
      "epoch: 256, loss: 0.909\n",
      "epoch: 257, loss: 0.908\n",
      "epoch: 258, loss: 0.903\n",
      "epoch: 259, loss: 0.894\n",
      "epoch: 260, loss: 0.892\n",
      "epoch: 261, loss: 0.926\n",
      "epoch: 262, loss: 0.886\n",
      "epoch: 263, loss: 0.896\n",
      "epoch: 264, loss: 0.909\n",
      "epoch: 265, loss: 0.894\n",
      "epoch: 266, loss: 0.923\n",
      "epoch: 267, loss: 0.891\n",
      "epoch: 268, loss: 0.88\n",
      "epoch: 269, loss: 0.879\n",
      "epoch: 270, loss: 0.847\n",
      "epoch: 271, loss: 0.908\n",
      "epoch: 272, loss: 0.907\n",
      "epoch: 273, loss: 0.893\n",
      "epoch: 274, loss: 0.878\n",
      "epoch: 275, loss: 0.872\n",
      "epoch: 276, loss: 0.876\n",
      "epoch: 277, loss: 0.884\n",
      "epoch: 278, loss: 0.921\n",
      "epoch: 279, loss: 0.892\n",
      "epoch: 280, loss: 0.886\n",
      "epoch: 281, loss: 0.864\n",
      "epoch: 282, loss: 0.875\n",
      "epoch: 283, loss: 0.861\n",
      "epoch: 284, loss: 0.876\n",
      "epoch: 285, loss: 0.846\n",
      "epoch: 286, loss: 0.856\n",
      "epoch: 287, loss: 0.896\n",
      "epoch: 288, loss: 0.878\n",
      "epoch: 289, loss: 0.894\n",
      "epoch: 290, loss: 0.861\n",
      "epoch: 291, loss: 0.87\n",
      "epoch: 292, loss: 0.863\n",
      "epoch: 293, loss: 0.871\n",
      "epoch: 294, loss: 0.872\n",
      "epoch: 295, loss: 0.894\n",
      "epoch: 296, loss: 0.862\n",
      "epoch: 297, loss: 0.883\n",
      "epoch: 298, loss: 0.833\n",
      "epoch: 299, loss: 0.843\n",
      "epoch: 300, loss: 0.863\n",
      "epoch: 301, loss: 0.871\n",
      "epoch: 302, loss: 0.857\n",
      "epoch: 303, loss: 0.864\n",
      "epoch: 304, loss: 0.859\n",
      "epoch: 305, loss: 0.857\n",
      "epoch: 306, loss: 0.834\n",
      "epoch: 307, loss: 0.891\n",
      "epoch: 308, loss: 0.852\n",
      "epoch: 309, loss: 0.834\n",
      "epoch: 310, loss: 0.86\n",
      "epoch: 311, loss: 0.86\n",
      "epoch: 312, loss: 0.852\n",
      "epoch: 313, loss: 0.89\n",
      "epoch: 314, loss: 0.813\n",
      "epoch: 315, loss: 0.845\n",
      "epoch: 316, loss: 0.865\n",
      "epoch: 317, loss: 0.849\n",
      "epoch: 318, loss: 0.839\n",
      "epoch: 319, loss: 0.835\n",
      "epoch: 320, loss: 0.826\n",
      "epoch: 321, loss: 0.864\n",
      "epoch: 322, loss: 0.866\n",
      "epoch: 323, loss: 0.865\n",
      "epoch: 324, loss: 0.849\n",
      "epoch: 325, loss: 0.836\n",
      "epoch: 326, loss: 0.874\n",
      "epoch: 327, loss: 0.856\n",
      "epoch: 328, loss: 0.832\n",
      "epoch: 329, loss: 0.838\n",
      "epoch: 330, loss: 0.83\n",
      "epoch: 331, loss: 0.87\n",
      "epoch: 332, loss: 0.796\n",
      "epoch: 333, loss: 0.8\n",
      "epoch: 334, loss: 0.826\n",
      "epoch: 335, loss: 0.839\n",
      "epoch: 336, loss: 0.829\n",
      "epoch: 337, loss: 0.838\n",
      "epoch: 338, loss: 0.835\n",
      "epoch: 339, loss: 0.814\n",
      "epoch: 340, loss: 0.843\n",
      "epoch: 341, loss: 0.864\n",
      "epoch: 342, loss: 0.858\n",
      "epoch: 343, loss: 0.842\n",
      "epoch: 344, loss: 0.846\n",
      "epoch: 345, loss: 0.822\n",
      "epoch: 346, loss: 0.831\n",
      "epoch: 347, loss: 0.839\n",
      "epoch: 348, loss: 0.855\n",
      "epoch: 349, loss: 0.827\n",
      "epoch: 350, loss: 0.839\n",
      "epoch: 351, loss: 0.804\n",
      "epoch: 352, loss: 0.741\n",
      "epoch: 353, loss: 0.827\n",
      "epoch: 354, loss: 0.832\n",
      "epoch: 355, loss: 0.866\n",
      "epoch: 356, loss: 0.827\n",
      "epoch: 357, loss: 0.826\n",
      "epoch: 358, loss: 0.816\n",
      "epoch: 359, loss: 0.892\n",
      "epoch: 360, loss: 0.847\n",
      "epoch: 361, loss: 0.841\n",
      "epoch: 362, loss: 0.857\n",
      "epoch: 363, loss: 0.803\n",
      "epoch: 364, loss: 0.835\n",
      "epoch: 365, loss: 0.828\n",
      "epoch: 366, loss: 0.803\n",
      "epoch: 367, loss: 0.78\n",
      "epoch: 368, loss: 0.81\n",
      "epoch: 369, loss: 0.848\n",
      "epoch: 370, loss: 0.835\n",
      "epoch: 371, loss: 0.884\n",
      "epoch: 372, loss: 0.826\n",
      "epoch: 373, loss: 0.795\n",
      "epoch: 374, loss: 0.81\n",
      "epoch: 375, loss: 0.817\n",
      "epoch: 376, loss: 0.839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 377, loss: 0.845\n",
      "epoch: 378, loss: 0.798\n",
      "epoch: 379, loss: 0.829\n",
      "epoch: 380, loss: 0.847\n",
      "epoch: 381, loss: 0.842\n",
      "epoch: 382, loss: 0.845\n",
      "epoch: 383, loss: 0.812\n",
      "epoch: 384, loss: 0.868\n",
      "epoch: 385, loss: 0.81\n",
      "epoch: 386, loss: 0.879\n",
      "epoch: 387, loss: 0.851\n",
      "epoch: 388, loss: 0.859\n",
      "epoch: 389, loss: 0.853\n",
      "epoch: 390, loss: 0.786\n",
      "epoch: 391, loss: 0.793\n",
      "epoch: 392, loss: 0.798\n",
      "epoch: 393, loss: 0.821\n",
      "epoch: 394, loss: 0.849\n",
      "epoch: 395, loss: 0.8\n",
      "epoch: 396, loss: 0.784\n",
      "epoch: 397, loss: 0.819\n",
      "epoch: 398, loss: 0.846\n",
      "epoch: 399, loss: 0.793\n",
      "epoch: 400, loss: 0.829\n",
      "epoch: 401, loss: 0.817\n",
      "epoch: 402, loss: 0.817\n",
      "epoch: 403, loss: 0.807\n",
      "epoch: 404, loss: 0.809\n",
      "epoch: 405, loss: 0.872\n",
      "epoch: 406, loss: 0.846\n",
      "epoch: 407, loss: 0.854\n",
      "epoch: 408, loss: 0.814\n",
      "epoch: 409, loss: 0.797\n",
      "epoch: 410, loss: 0.846\n",
      "epoch: 411, loss: 0.807\n",
      "epoch: 412, loss: 0.757\n",
      "epoch: 413, loss: 0.822\n",
      "epoch: 414, loss: 0.862\n",
      "epoch: 415, loss: 0.773\n",
      "epoch: 416, loss: 0.817\n",
      "epoch: 417, loss: 0.8\n",
      "epoch: 418, loss: 0.822\n",
      "epoch: 419, loss: 0.781\n",
      "epoch: 420, loss: 0.789\n",
      "epoch: 421, loss: 0.776\n",
      "epoch: 422, loss: 0.814\n",
      "epoch: 423, loss: 0.784\n",
      "epoch: 424, loss: 0.837\n",
      "epoch: 425, loss: 0.779\n",
      "epoch: 426, loss: 0.794\n",
      "epoch: 427, loss: 0.805\n",
      "epoch: 428, loss: 0.782\n",
      "epoch: 429, loss: 0.776\n",
      "epoch: 430, loss: 0.815\n",
      "epoch: 431, loss: 0.796\n",
      "epoch: 432, loss: 0.822\n",
      "epoch: 433, loss: 0.805\n",
      "epoch: 434, loss: 0.786\n",
      "epoch: 435, loss: 0.791\n",
      "epoch: 436, loss: 0.772\n",
      "epoch: 437, loss: 0.827\n",
      "epoch: 438, loss: 0.82\n",
      "epoch: 439, loss: 0.798\n",
      "epoch: 440, loss: 0.812\n",
      "epoch: 441, loss: 0.798\n",
      "epoch: 442, loss: 0.831\n",
      "epoch: 443, loss: 0.824\n",
      "epoch: 444, loss: 0.859\n",
      "epoch: 445, loss: 0.858\n",
      "epoch: 446, loss: 0.86\n",
      "epoch: 447, loss: 0.76\n",
      "epoch: 448, loss: 0.838\n",
      "epoch: 449, loss: 0.792\n",
      "epoch: 450, loss: 0.804\n",
      "epoch: 451, loss: 0.8\n",
      "epoch: 452, loss: 0.837\n",
      "epoch: 453, loss: 0.803\n",
      "epoch: 454, loss: 0.842\n",
      "epoch: 455, loss: 0.79\n",
      "epoch: 456, loss: 0.808\n",
      "epoch: 457, loss: 0.818\n",
      "epoch: 458, loss: 0.756\n",
      "epoch: 459, loss: 0.804\n",
      "epoch: 460, loss: 0.807\n",
      "epoch: 461, loss: 0.775\n",
      "epoch: 462, loss: 0.781\n",
      "epoch: 463, loss: 0.857\n",
      "epoch: 464, loss: 0.83\n",
      "epoch: 465, loss: 0.823\n",
      "epoch: 466, loss: 0.796\n",
      "epoch: 467, loss: 0.783\n",
      "epoch: 468, loss: 0.765\n",
      "epoch: 469, loss: 0.789\n",
      "epoch: 470, loss: 0.798\n",
      "epoch: 471, loss: 0.829\n",
      "epoch: 472, loss: 0.764\n",
      "epoch: 473, loss: 0.845\n",
      "epoch: 474, loss: 0.797\n",
      "epoch: 475, loss: 0.784\n",
      "epoch: 476, loss: 0.791\n",
      "epoch: 477, loss: 0.73\n",
      "epoch: 478, loss: 0.826\n",
      "epoch: 479, loss: 0.796\n",
      "epoch: 480, loss: 0.82\n",
      "epoch: 481, loss: 0.806\n",
      "epoch: 482, loss: 0.77\n",
      "epoch: 483, loss: 0.81\n",
      "epoch: 484, loss: 0.816\n",
      "epoch: 485, loss: 0.813\n",
      "epoch: 486, loss: 0.766\n",
      "epoch: 487, loss: 0.769\n",
      "epoch: 488, loss: 0.797\n",
      "epoch: 489, loss: 0.807\n",
      "epoch: 490, loss: 0.776\n",
      "epoch: 491, loss: 0.786\n",
      "epoch: 492, loss: 0.788\n",
      "epoch: 493, loss: 0.832\n",
      "epoch: 494, loss: 0.78\n",
      "epoch: 495, loss: 0.808\n",
      "epoch: 496, loss: 0.791\n",
      "epoch: 497, loss: 0.804\n",
      "epoch: 498, loss: 0.829\n",
      "epoch: 499, loss: 0.789\n",
      "epoch: 500, loss: 0.814\n"
     ]
    }
   ],
   "source": [
    "model=MLP(2,3,1).to(device)\n",
    "\n",
    "criterion=nn.BCELoss()\n",
    "optimizer=optimizers.SGD(model.parameters(),lr=0.1)\n",
    "\n",
    "def compute_loss(t,y):\n",
    "    return criterion(y,t)\n",
    "\n",
    "def train_step(x,t):\n",
    "    model.train()\n",
    "    preds=model(x)\n",
    "    loss=compute_loss(t,preds)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "epochs=500\n",
    "batch_size=100\n",
    "n_batches=x_train.shape[0]//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss=0.\n",
    "    x_,t_=shuffle(x_train,t_train)\n",
    "    x_=torch.Tensor(x_).to(device)\n",
    "    t_=torch.Tensor(t_).to(device)\n",
    "    \n",
    "    for n_batch in range(n_batches):\n",
    "        start=n_batch*batch_size\n",
    "        end=start+batch_size\n",
    "        loss=train_step(x_[start:end],t_[start:end])\n",
    "        train_loss+=loss.item()\n",
    "    print('epoch: {}, loss: {:.3}'.format(epoch+1,train_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.331, test_acc: 0.867\n"
     ]
    }
   ],
   "source": [
    "def test_step(x,t):\n",
    "    x=torch.Tensor(x).to(device)\n",
    "    t=torch.Tensor(t).to(device)\n",
    "    model.eval()\n",
    "    preds=model(x)\n",
    "    loss=compute_loss(t,preds)\n",
    "    return loss,preds\n",
    "\n",
    "loss,preds=test_step(x_test,t_test)\n",
    "test_loss=loss.item()\n",
    "preds=preds.data.cpu().numpy()>0.5\n",
    "test_acc=accuracy_score(t_test,preds)\n",
    "print('test_loss: {:.3f}, test_acc: {:.3f}'.format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
